{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cloudy-schedule",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-06-09T08:08:42.846187Z",
     "iopub.status.busy": "2021-06-09T08:08:42.845821Z",
     "iopub.status.idle": "2021-06-09T08:10:23.759465Z",
     "shell.execute_reply": "2021-06-09T08:10:23.758484Z",
     "shell.execute_reply.started": "2021-06-09T08:08:42.846105Z"
    },
    "id": "ptkHaiID_EWG",
    "papermill": {
     "duration": 0.087484,
     "end_time": "2021-06-13T15:26:14.738261",
     "exception": false,
     "start_time": "2021-06-13T15:26:14.650777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Machine Learning approach to predict real or fake tweets about disaster**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "speaking-engine",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 18.967857,
     "end_time": "2021-06-13T15:26:34.460944",
     "exception": false,
     "start_time": "2021-06-13T15:26:15.493087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\harish-4072\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chart_studio in c:\\users\\harish-4072\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\harish-4072\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chart_studio) (1.3.4)\n",
      "Requirement already satisfied: plotly in c:\\users\\harish-4072\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chart_studio) (5.24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\harish-4072\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chart_studio) (2.32.3)\n",
      "Requirement already satisfied: six in c:\\users\\harish-4072\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chart_studio) (1.16.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\harish-4072\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from plotly->chart_studio) (24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\harish-4072\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from plotly->chart_studio) (9.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harish-4072\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->chart_studio) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harish-4072\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->chart_studio) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harish-4072\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->chart_studio) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harish-4072\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->chart_studio) (2024.8.30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "#libraries for NLP\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython.display import HTML\n",
    "!pip install chart_studio\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import chart_studio.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import iplot\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import plotly.express as px\n",
    "from collections import defaultdict\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-testing",
   "metadata": {
    "papermill": {
     "duration": 0.086936,
     "end_time": "2021-06-13T15:26:34.635641",
     "exception": false,
     "start_time": "2021-06-13T15:26:34.548705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Reading and preparation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-salvation",
   "metadata": {
    "papermill": {
     "duration": 0.086489,
     "end_time": "2021-06-13T15:26:34.809004",
     "exception": false,
     "start_time": "2021-06-13T15:26:34.722515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Reading [data](https://www.kaggle.com/c/nlp-getting-started/data) and choosing important columns using [pandas](https://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polish-jumping",
   "metadata": {
    "id": "maaZD2m7GrYZ",
    "papermill": {
     "duration": 0.141549,
     "end_time": "2021-06-13T15:26:35.037696",
     "exception": false,
     "start_time": "2021-06-13T15:26:34.896147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"D:\\elggak\\kaggle\\Tweet Disaster Competition\\nlp-getting-started\\train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-suspension",
   "metadata": {
    "papermill": {
     "duration": 0.087618,
     "end_time": "2021-06-13T15:26:35.213279",
     "exception": false,
     "start_time": "2021-06-13T15:26:35.125661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Displaying first 10 rows of our data using [DataFrame.head()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "literary-prague",
   "metadata": {
    "id": "pYXi7Ltic6iG",
    "outputId": "a0ba0fd6-5162-4946-87b6-e07d4932dce1",
    "papermill": {
     "duration": 0.105925,
     "end_time": "2021-06-13T15:26:36.220013",
     "exception": false,
     "start_time": "2021-06-13T15:26:36.114088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['text','target']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convertible-imaging",
   "metadata": {
    "id": "sckzCfiLegfL",
    "papermill": {
     "duration": 14.87848,
     "end_time": "2021-06-13T15:27:02.742698",
     "exception": false,
     "start_time": "2021-06-13T15:26:47.864218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \n",
    "    #removal of url\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ',data) \n",
    "    \n",
    "    #decontraction\n",
    "    text = re.sub(r\"won\\'t\", \" will not\", text)\n",
    "    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n",
    "    text = re.sub(r\"can\\'t\", \" can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \" do not\", text)    \n",
    "    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n",
    "    text = re.sub(r\"ma\\'am\", \" madam\", text)\n",
    "    text = re.sub(r\"let\\'s\", \" let us\", text)\n",
    "    text = re.sub(r\"ain\\'t\", \" am not\", text)\n",
    "    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n",
    "    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n",
    "    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n",
    "    text = re.sub(r\"y\\'all\", \" you all\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"n\\'t've\", \" not have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'d've\", \" would have\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ll've\", \" will have\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    \n",
    "    #removal of html tags\n",
    "    text = re.sub(r'<.*?>',' ',text) \n",
    "    \n",
    "    # Match all digits in the string and replace them by empty string\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    text = re.sub(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\",' ',text)\n",
    "    \n",
    "    # filtering out miscellaneous text.\n",
    "    text = re.sub('[^a-zA-Z]',' ',text) \n",
    "    text = re.sub(r\"\\([^()]*\\)\", \"\", text)\n",
    "    \n",
    "    # remove mentions\n",
    "    text = re.sub('@\\S+', '', text)  \n",
    "    \n",
    "    # remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', text)  \n",
    "    \n",
    "\n",
    "    # Lowering all the words in text\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    \n",
    "    text = [stemmer.stem(words) for words in text if words not in stopwords.words('english')]\n",
    "    \n",
    "    # Removal of words with length<2\n",
    "    text = [i for i in text if len(i)>2] \n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "data[\"Cleaned_text\"] = data[\"text\"].apply(preprocess_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-affairs",
   "metadata": {
    "papermill": {
     "duration": 0.214498,
     "end_time": "2021-06-13T15:27:03.167469",
     "exception": false,
     "start_time": "2021-06-13T15:27:02.952971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Displaying Cleaned Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "perceived-sword",
   "metadata": {
    "id": "iwX5VY5bK13S",
    "outputId": "7ecbefb1-5c8c-4e22-aebb-8264be851a15",
    "papermill": {
     "duration": 0.265663,
     "end_time": "2021-06-13T15:27:03.644211",
     "exception": false,
     "start_time": "2021-06-13T15:27:03.378548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask ishelt place notifi offic evacu shel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                        Cleaned_text  \n",
       "0             deed reason earthquak may allah forgiv  \n",
       "1                  forest fire near rong sask canada  \n",
       "2  resid ask ishelt place notifi offic evacu shel...  \n",
       "3        peopl receiv wildfir evacu order california  \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lightweight-brazil",
   "metadata": {
    "id": "-_-USeyzNoPr",
    "papermill": {
     "duration": 0.275324,
     "end_time": "2021-06-13T15:27:11.805944",
     "exception": false,
     "start_time": "2021-06-13T15:27:11.530620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_words = ['via','like','build','get','would','one','two','feel','lol','fuck','take','way','may','first','latest'\n",
    "                'want','make','back','see','know','let','look','come','got','still','say','think','great','pleas','amp']\n",
    "\n",
    "def text_cleaning(data):\n",
    "    return ' '.join(i for i in data.split() if i not in common_words)\n",
    "\n",
    "data[\"Cleaned_text\"] = data[\"Cleaned_text\"].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-nevada",
   "metadata": {
    "papermill": {
     "duration": 0.231758,
     "end_time": "2021-06-13T15:27:17.165308",
     "exception": false,
     "start_time": "2021-06-13T15:27:16.933550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-circulation",
   "metadata": {
    "papermill": {
     "duration": 0.22656,
     "end_time": "2021-06-13T15:27:17.620134",
     "exception": false,
     "start_time": "2021-06-13T15:27:17.393574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.1 Spliting original data after cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reasonable-patch",
   "metadata": {
    "papermill": {
     "duration": 0.236981,
     "end_time": "2021-06-13T15:27:18.084303",
     "exception": false,
     "start_time": "2021-06-13T15:27:17.847322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_inp_clean = data['Cleaned_text']\n",
    "X_inp_original = data['text']\n",
    "y_inp = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-digest",
   "metadata": {
    "papermill": {
     "duration": 0.235044,
     "end_time": "2021-06-13T15:27:18.549434",
     "exception": false,
     "start_time": "2021-06-13T15:27:18.314390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Using [scikit-learn's train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the data into training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "environmental-brief",
   "metadata": {
    "papermill": {
     "duration": 0.246245,
     "end_time": "2021-06-13T15:27:19.022872",
     "exception": false,
     "start_time": "2021-06-13T15:27:18.776627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_inp_clean, y_inp, test_size=0.2, random_state=42, stratify=y_inp)\n",
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-growing",
   "metadata": {
    "papermill": {
     "duration": 0.22577,
     "end_time": "2021-06-13T15:27:19.474589",
     "exception": false,
     "start_time": "2021-06-13T15:27:19.248819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "checking size of data after train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "posted-dakota",
   "metadata": {
    "papermill": {
     "duration": 0.239556,
     "end_time": "2021-06-13T15:27:19.939753",
     "exception": false,
     "start_time": "2021-06-13T15:27:19.700197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6090,), (1523,), (6090,), (1523,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "808de274-4b48-4f89-826d-c65f936f6215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on TF-IDF dataset...\n",
      "Naive Bayes Accuracy: 0.7977675640183848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.792355  0.877574  0.832790       874\n",
      "        spam   0.807207  0.690293  0.744186       649\n",
      "\n",
      "    accuracy                       0.797768      1523\n",
      "   macro avg   0.799781  0.783934  0.788488      1523\n",
      "weighted avg   0.798684  0.797768  0.795033      1523\n",
      "\n",
      "Logistic Regression Accuracy: 0.7892317793827971\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.774578  0.892449  0.829346       874\n",
      "        spam   0.817829  0.650231  0.724464       649\n",
      "\n",
      "    accuracy                       0.789232      1523\n",
      "   macro avg   0.796204  0.771340  0.776905      1523\n",
      "weighted avg   0.793009  0.789232  0.784652      1523\n",
      "\n",
      "SVM Accuracy: 0.7925147734734077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.770349  0.909611  0.834208       874\n",
      "        spam   0.839104  0.634823  0.722807       649\n",
      "\n",
      "    accuracy                       0.792515      1523\n",
      "   macro avg   0.804726  0.772217  0.778507      1523\n",
      "weighted avg   0.799648  0.792515  0.786736      1523\n",
      "\n",
      "Ensemble Model Accuracy: 0.7944845699277742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.776900  0.900458  0.834128       874\n",
      "        spam   0.829412  0.651772  0.729940       649\n",
      "\n",
      "    accuracy                       0.794485      1523\n",
      "   macro avg   0.803156  0.776115  0.782034      1523\n",
      "weighted avg   0.799277  0.794485  0.789730      1523\n",
      "\n",
      "Running models on Count Vectorization dataset...\n",
      "Naive Bayes Accuracy: 0.7872619829284307\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.816092  0.812357  0.814220       874\n",
      "        spam   0.748851  0.753467  0.751152       649\n",
      "\n",
      "    accuracy                       0.787262      1523\n",
      "   macro avg   0.782472  0.782912  0.782686      1523\n",
      "weighted avg   0.787439  0.787262  0.787345      1523\n",
      "\n",
      "Logistic Regression Accuracy: 0.7866053841103086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.794844  0.846682  0.819945       874\n",
      "        spam   0.773649  0.705701  0.738114       649\n",
      "\n",
      "    accuracy                       0.786605      1523\n",
      "   macro avg   0.784246  0.776192  0.779030      1523\n",
      "weighted avg   0.785812  0.786605  0.785074      1523\n",
      "\n",
      "SVM Accuracy: 0.8049901510177282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.789950  0.899314  0.841091       874\n",
      "        spam   0.833333  0.677966  0.747664       649\n",
      "\n",
      "    accuracy                       0.804990      1523\n",
      "   macro avg   0.811642  0.788640  0.794378      1523\n",
      "weighted avg   0.808437  0.804990  0.801279      1523\n",
      "\n",
      "Ensemble Model Accuracy: 0.793827971109652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.795983  0.861556  0.827473       874\n",
      "        spam   0.790295  0.702619  0.743883       649\n",
      "\n",
      "    accuracy                       0.793828      1523\n",
      "   macro avg   0.793139  0.782088  0.785678      1523\n",
      "weighted avg   0.793559  0.793828  0.791852      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tfid_preprocessing(train_data, train_target):\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "    train_data = vectorizer.fit_transform(train_data)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_data, train_target.values, test_size=0.2, random_state=42)\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "def count_preprocessing(train_data, train_target):\n",
    "    vectorizer = CountVectorizer()\n",
    "    train_data = vectorizer.fit_transform(train_data)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_data, train_target.values, test_size=0.2, random_state=42)\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "def naive_bayes(X_train, X_val, y_train, y_val):\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    y_pred = nb_model.predict(X_val)\n",
    "    print(f'Accuracy: {accuracy_score(y_val, y_pred)}')\n",
    "    print(classification_report(y_val, y_pred, target_names=['ham', 'spam'], digits=6))\n",
    "   \n",
    "\n",
    "def logistic_regression(X_train, X_val, y_train, y_val):\n",
    "    lr_model = LogisticRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    y_pred = lr_model.predict(X_val)\n",
    "    print(f'Accuracy: {accuracy_score(y_val, y_pred)}')\n",
    "    print(classification_report(y_val, y_pred, target_names=['ham', 'spam'], digits=6))\n",
    "   \n",
    "\n",
    "def svm_classification(X_train, X_val, y_train, y_val):\n",
    "    svm_model = SVC(kernel='rbf')\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    y_pred = svm_model.predict(X_val)\n",
    "    print(f'Accuracy: {accuracy_score(y_val, y_pred)}')\n",
    "    print(classification_report(y_val, y_pred, target_names=['ham', 'spam'], digits=6))\n",
    "\n",
    "\n",
    "def run_models(X_train, X_val, y_train, y_val, model_name):\n",
    "    print(f\"Running models on {model_name} dataset...\")\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    y_pred_nb = nb_model.predict(X_val)\n",
    "    print(f'Naive Bayes Accuracy: {accuracy_score(y_val, y_pred_nb)}')\n",
    "    print(classification_report(y_val, y_pred_nb, target_names=['ham', 'spam'], digits=6))\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr_model = LogisticRegression(C=1.0,max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    y_pred_lr = lr_model.predict(X_val)\n",
    "    print(f'Logistic Regression Accuracy: {accuracy_score(y_val, y_pred_lr)}')\n",
    "    print(classification_report(y_val, y_pred_lr, target_names=['ham', 'spam'], digits=6))\n",
    "\n",
    "    # SVM\n",
    "    svm_model = SVC(kernel='rbf')\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    y_pred_svm = svm_model.predict(X_val)\n",
    "    print(f'SVM Accuracy: {accuracy_score(y_val, y_pred_svm)}')\n",
    "    print(classification_report(y_val, y_pred_svm, target_names=['ham', 'spam'], digits=6))\n",
    "\n",
    "    ensemble_model = VotingClassifier(estimators=[\n",
    "        ('nb', nb_model),\n",
    "        ('lr', lr_model),\n",
    "        ('svm', svm_model)\n",
    "    ], voting='hard')  # Hard voting for majority vote\n",
    "\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    y_pred_ensemble = ensemble_model.predict(X_val)\n",
    "    print(f'Ensemble Model Accuracy: {accuracy_score(y_val, y_pred_ensemble)}')\n",
    "    print(classification_report(y_val, y_pred_ensemble, target_names=['ham', 'spam'], digits=6))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run the pipeline for TF-IDF\n",
    "    X_train_tfidf, X_val_tfidf, y_train_tfidf, y_val_tfidf = tfid_preprocessing(X_inp_clean,y_inp)\n",
    "    run_models(X_train_tfidf, X_val_tfidf, y_train_tfidf, y_val_tfidf, \"TF-IDF\")\n",
    "\n",
    "    # # Run the pipeline for Count Vectorization\n",
    "    X_train_count, X_val_count, y_train_count, y_val_count = count_preprocessing(X_inp_clean,y_inp)\n",
    "    run_models(X_train_count, X_val_count, y_train_count, y_val_count, \"Count Vectorization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-hebrew",
   "metadata": {
    "papermill": {
     "duration": 0.228234,
     "end_time": "2021-06-13T15:27:20.395540",
     "exception": false,
     "start_time": "2021-06-13T15:27:20.167306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.2 Creating function to encode data using BoW or TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-values",
   "metadata": {
    "papermill": {
     "duration": 0.228521,
     "end_time": "2021-06-13T15:27:20.851396",
     "exception": false,
     "start_time": "2021-06-13T15:27:20.622875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### What is BoW?   \n",
    "BoW stands for \"*bag of words*\" which is a representation of text that describes the occurrence of words within a document.   \n",
    "We just keep track of word counts and disregard the grammatical details and the word order.   \n",
    "It is called a “bag” of words because any information about the order or structure of words in the document is discarded. \n",
    "The model is only concerned with whether known words occur in the document, not where in the document.\n",
    "    \n",
    "    \n",
    "### What is TF-IDF?\n",
    "TF-IDF which means Term Frequency and Inverse Document Frequency, is a scoring measure widely used in information retrieval (IR) or summarization.     \n",
    "TF-IDF is intended to reflect how relevant a term is in a given document. It is a technique in Natural Language Processing for converting words in Vectors and with some semantic information and it gives weighted to uncommon words , used in various NLP applications.    \n",
    "\n",
    "For [BoW](https://www.mygreatlearning.com/blog/bag-of-words/) approach we use scikit-learn's [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and for [TF-IDF](https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76) we use [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "random-current",
   "metadata": {
    "id": "-Cw5J87AnyN4",
    "papermill": {
     "duration": 0.240863,
     "end_time": "2021-06-13T15:27:21.321138",
     "exception": false,
     "start_time": "2021-06-13T15:27:21.080275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encoding(train_data,valid_data,bow=False,n=1,tf_idf=False):\n",
    "    if bow==True:\n",
    "        cv = CountVectorizer(ngram_range=(n,n))\n",
    "        cv_df_train = cv.fit_transform(train_data).toarray()\n",
    "        train_df = pd.DataFrame(cv_df_train,columns=cv.get_feature_names())\n",
    "        cv_df_valid = cv.transform(valid_data).toarray()\n",
    "        valid_df = pd.DataFrame(cv_df_valid,columns=cv.get_feature_names())\n",
    "        \n",
    "    elif tf_idf==True:\n",
    "        \n",
    "        tfidf = TfidfVectorizer(ngram_range=(n, n), use_idf=1,smooth_idf=1,sublinear_tf=1)    \n",
    "        tf_df_train = tfidf.fit_transform(train_data).toarray()\n",
    "        train_df = pd.DataFrame(tf_df_train,columns=tfidf.get_feature_names())\n",
    "        tf_df_valid = tfidf.transform(valid_data).toarray()\n",
    "        valid_df = pd.DataFrame(tf_df_valid,columns=tfidf.get_feature_names())\n",
    "        \n",
    "    return train_df,valid_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-cornwall",
   "metadata": {
    "papermill": {
     "duration": 0.227488,
     "end_time": "2021-06-13T15:27:21.777032",
     "exception": false,
     "start_time": "2021-06-13T15:27:21.549544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.3 Encoding training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-jacksonville",
   "metadata": {
    "papermill": {
     "duration": 0.232687,
     "end_time": "2021-06-13T15:27:22.239397",
     "exception": false,
     "start_time": "2021-06-13T15:27:22.006710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We encode our data in all possible combinations provided by our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vertical-privilege",
   "metadata": {
    "papermill": {
     "duration": 2.673646,
     "end_time": "2021-06-13T15:27:25.141489",
     "exception": false,
     "start_time": "2021-06-13T15:27:22.467843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_bow1 , X_valid_bow1 \u001b[38;5;241m=\u001b[39m \u001b[43mencoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[0;32m      2\u001b[0m X_train_bow2 , X_valid_bow2 \u001b[38;5;241m=\u001b[39m encoding(X_train,X_valid,bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \n\u001b[0;32m      3\u001b[0m X_train_bow3 , X_valid_bow3 \u001b[38;5;241m=\u001b[39m encoding(X_train,X_valid,bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m) \n",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m, in \u001b[0;36mencoding\u001b[1;34m(train_data, valid_data, bow, n, tf_idf)\u001b[0m\n\u001b[0;32m      3\u001b[0m cv \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(n,n))\n\u001b[0;32m      4\u001b[0m cv_df_train \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mfit_transform(train_data)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m----> 5\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(cv_df_train,columns\u001b[38;5;241m=\u001b[39m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m())\n\u001b[0;32m      6\u001b[0m cv_df_valid \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mtransform(valid_data)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      7\u001b[0m valid_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(cv_df_valid,columns\u001b[38;5;241m=\u001b[39mcv\u001b[38;5;241m.\u001b[39mget_feature_names())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "X_train_bow1 , X_valid_bow1 = encoding(X_train,X_valid,bow=True) \n",
    "X_train_bow2 , X_valid_bow2 = encoding(X_train,X_valid,bow=True,n=2) \n",
    "X_train_bow3 , X_valid_bow3 = encoding(X_train,X_valid,bow=True,n=3) \n",
    "X_train_tfidf1 , X_valid_tfidf1 = encoding(X_train,X_valid,tf_idf=True) \n",
    "X_train_tfidf2 , X_valid_tfidf2 = encoding(X_train,X_valid,tf_idf=True,n=2) \n",
    "X_train_tfidf3 , X_valid_tfidf3 = encoding(X_train,X_valid,tf_idf=True,n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-canvas",
   "metadata": {
    "papermill": {
     "duration": 0.228851,
     "end_time": "2021-06-13T15:27:25.600602",
     "exception": false,
     "start_time": "2021-06-13T15:27:25.371751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Training and tuning Machine Learining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-aluminum",
   "metadata": {
    "papermill": {
     "duration": 0.231934,
     "end_time": "2021-06-13T15:27:26.064383",
     "exception": false,
     "start_time": "2021-06-13T15:27:25.832449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### What is a classification report?\n",
    "A Classification report is used to measure the quality of predictions from a classification algorithm.   \n",
    "The report shows the main classification metrics precision, recall and f1-score on a per-class basis. The metrics are calculated by using true and false positives, true and false negatives. Positive and negative in this case are generic names for the predicted classes. \n",
    "### What is a confusion matrix?\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.  \n",
    "\n",
    "#### In a confusion matrix there are 4 basic terminologies :\n",
    "* **true positives (TP)** : We predicted yes (they are real tweets), and they are actually real.\n",
    "* **true negatives (TN)** : We predicted no, and they are fake.\n",
    "* **false positives (FP)**: We predicted yes, but they are't actually real. (Also known as a \"Type I error.\")\n",
    "* **false negatives (FN)**: We predicted no, but they are real. (Also known as a \"Type II error.\")\n",
    "\n",
    "Now let's create functions to display model's [classification report](https://datascience.stackexchange.com/questions/64441/how-to-interpret-classification-report-of-scikit-learn) and [confusion matrix](https://machinelearningmastery.com/confusion-matrix-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-trade",
   "metadata": {
    "papermill": {
     "duration": 0.240061,
     "end_time": "2021-06-13T15:27:26.535256",
     "exception": false,
     "start_time": "2021-06-13T15:27:26.295195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def c_report(y_true,y_pred):\n",
    "    print(\"Classifictaion Report\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    acc_scr = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy : \"+ str(acc_scr))\n",
    "    return acc_scr\n",
    "\n",
    "def plot_cm(y_true,y_pred,cmap = \"Blues\"):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(mtx, annot = True, fmt='d', linewidth=0.5,\n",
    "               cmap=cmap, cbar = False)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-congo",
   "metadata": {
    "papermill": {
     "duration": 0.27104,
     "end_time": "2021-06-13T15:27:27.034391",
     "exception": false,
     "start_time": "2021-06-13T15:27:26.763351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-cleaner",
   "metadata": {
    "papermill": {
     "duration": 0.228686,
     "end_time": "2021-06-13T15:27:27.491701",
     "exception": false,
     "start_time": "2021-06-13T15:27:27.263015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### About Logistic Regression   \n",
    "\n",
    "Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).\n",
    "\n",
    "Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n",
    "\n",
    "Now let's create a [Logistic Regression](https://medium.com/analytics-vidhya/applying-text-classification-using-logistic-regression-a-comparison-between-bow-and-tf-idf-1f1ed1b83640) model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-strip",
   "metadata": {
    "papermill": {
     "duration": 6.014244,
     "end_time": "2021-06-13T15:27:33.735093",
     "exception": false,
     "start_time": "2021-06-13T15:27:27.720849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bow1_logreg = LogisticRegression()\n",
    "model_bow1_logreg.fit(X_train_bow1,y_train)\n",
    "pred_bow1_logreg = model_bow1_logreg.predict(X_valid_bow1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-hydrogen",
   "metadata": {
    "papermill": {
     "duration": 0.22907,
     "end_time": "2021-06-13T15:27:34.241988",
     "exception": false,
     "start_time": "2021-06-13T15:27:34.012918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(BoW,n-grams=1) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-provision",
   "metadata": {
    "papermill": {
     "duration": 0.399098,
     "end_time": "2021-06-13T15:27:34.869600",
     "exception": false,
     "start_time": "2021-06-13T15:27:34.470502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_bow1_logreg = c_report(y_valid,pred_bow1_logreg)\n",
    "plot_cm(y_valid,pred_bow1_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-tomato",
   "metadata": {
    "papermill": {
     "duration": 0.231973,
     "end_time": "2021-06-13T15:27:35.340842",
     "exception": false,
     "start_time": "2021-06-13T15:27:35.108869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now training another Logistic Regression model with n-grams=2 and BoW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-peripheral",
   "metadata": {
    "papermill": {
     "duration": 11.596431,
     "end_time": "2021-06-13T15:27:47.167468",
     "exception": false,
     "start_time": "2021-06-13T15:27:35.571037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bow2_logreg = LogisticRegression()\n",
    "model_bow2_logreg.fit(X_train_bow2,y_train)\n",
    "pred_bow2_logreg = model_bow2_logreg.predict(X_valid_bow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-creek",
   "metadata": {
    "papermill": {
     "duration": 0.236741,
     "end_time": "2021-06-13T15:27:47.686081",
     "exception": false,
     "start_time": "2021-06-13T15:27:47.449340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(BoW,n-grams=2) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-isolation",
   "metadata": {
    "papermill": {
     "duration": 0.375166,
     "end_time": "2021-06-13T15:27:48.291812",
     "exception": false,
     "start_time": "2021-06-13T15:27:47.916646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_bow2_logreg = c_report(y_valid,pred_bow2_logreg)\n",
    "plot_cm(y_valid,pred_bow2_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-morris",
   "metadata": {
    "papermill": {
     "duration": 0.230668,
     "end_time": "2021-06-13T15:27:48.757804",
     "exception": false,
     "start_time": "2021-06-13T15:27:48.527136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can observe that as n is increasing model accuracy is decreasing   \n",
    "let's try to increase n one last time just to be sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-associate",
   "metadata": {
    "papermill": {
     "duration": 9.938016,
     "end_time": "2021-06-13T15:27:58.928645",
     "exception": false,
     "start_time": "2021-06-13T15:27:48.990629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bow3_logreg = LogisticRegression()\n",
    "model_bow3_logreg.fit(X_train_bow3,y_train)\n",
    "pred_bow3_logreg = model_bow3_logreg.predict(X_valid_bow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-religious",
   "metadata": {
    "papermill": {
     "duration": 0.233443,
     "end_time": "2021-06-13T15:27:59.448971",
     "exception": false,
     "start_time": "2021-06-13T15:27:59.215528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(BoW,n-grams=3) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-nowhere",
   "metadata": {
    "papermill": {
     "duration": 0.376984,
     "end_time": "2021-06-13T15:28:00.059328",
     "exception": false,
     "start_time": "2021-06-13T15:27:59.682344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_bow3_logreg = c_report(y_valid,pred_bow3_logreg)\n",
    "plot_cm(y_valid,pred_bow3_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-struggle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T08:11:24.298447Z",
     "iopub.status.busy": "2021-06-09T08:11:24.298095Z",
     "iopub.status.idle": "2021-06-09T08:11:24.306226Z",
     "shell.execute_reply": "2021-06-09T08:11:24.305086Z",
     "shell.execute_reply.started": "2021-06-09T08:11:24.29841Z"
    },
    "papermill": {
     "duration": 0.235929,
     "end_time": "2021-06-13T15:28:00.578611",
     "exception": false,
     "start_time": "2021-06-13T15:28:00.342682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "From the above results it's clear that using n = 1 will always give us more accuray,  \n",
    "now let's use tfidf approach with n = 1 to train our Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-shooting",
   "metadata": {
    "papermill": {
     "duration": 3.128408,
     "end_time": "2021-06-13T15:28:03.938817",
     "exception": false,
     "start_time": "2021-06-13T15:28:00.810409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tfidf1_logreg = LogisticRegression(C=1.0)\n",
    "model_tfidf1_logreg.fit(X_train_tfidf1,y_train)\n",
    "pred_tfidf1_logreg = model_tfidf1_logreg.predict(X_valid_tfidf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-tracker",
   "metadata": {
    "papermill": {
     "duration": 0.234482,
     "end_time": "2021-06-13T15:28:04.456836",
     "exception": false,
     "start_time": "2021-06-13T15:28:04.222354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(TF-IDF,n-grams=1) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-grass",
   "metadata": {
    "papermill": {
     "duration": 0.341235,
     "end_time": "2021-06-13T15:28:05.032557",
     "exception": false,
     "start_time": "2021-06-13T15:28:04.691322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_tfidf1_logreg = c_report(y_valid,pred_tfidf1_logreg)\n",
    "plot_cm(y_valid,pred_tfidf1_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-flower",
   "metadata": {
    "papermill": {
     "duration": 0.23522,
     "end_time": "2021-06-13T15:28:05.503606",
     "exception": false,
     "start_time": "2021-06-13T15:28:05.268386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "From Logistic Regression we saw n-grams = 1 gives the best results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-catalog",
   "metadata": {
    "papermill": {
     "duration": 0.232464,
     "end_time": "2021-06-13T15:28:05.974625",
     "exception": false,
     "start_time": "2021-06-13T15:28:05.742161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.2 Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-relay",
   "metadata": {
    "papermill": {
     "duration": 0.234059,
     "end_time": "2021-06-13T15:28:06.442598",
     "exception": false,
     "start_time": "2021-06-13T15:28:06.208539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### About Multinomial Naive Bayes  \n",
    "\n",
    "Multinomial Naive Bayes algorithm is a probabilistic learning method that is mostly used in Natural Language Processing (NLP). The algorithm is based on the Bayes theorem and predicts the tag of a text such as a piece of email or newspaper article. It calculates the probability of each tag for a given sample and then gives the tag with the highest probability as output.\n",
    "\n",
    "Naive Bayes classifier is a collection of many algorithms where all the algorithms share one common principle, and that is each feature being classified is not related to any other feature. The presence or absence of a feature does not affect the presence or absence of the other feature.\n",
    "    \n",
    "Now let's create [MultinomialNB](https://www.upgrad.com/blog/multinomial-naive-bayes-explained/) model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-virgin",
   "metadata": {
    "papermill": {
     "duration": 2.088335,
     "end_time": "2021-06-13T15:28:08.765231",
     "exception": false,
     "start_time": "2021-06-13T15:28:06.676896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bow1_NB = MultinomialNB(alpha=0.7)\n",
    "model_bow1_NB.fit(X_train_bow1,y_train)\n",
    "pred_bow1_NB = model_bow1_NB.predict(X_valid_bow1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-present",
   "metadata": {
    "papermill": {
     "duration": 0.234655,
     "end_time": "2021-06-13T15:28:09.284218",
     "exception": false,
     "start_time": "2021-06-13T15:28:09.049563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Printing classification report and ploting confusion matrix  for the predictions of MultinomialNB(BoW,n-grams=1) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-majority",
   "metadata": {
    "papermill": {
     "duration": 0.333449,
     "end_time": "2021-06-13T15:28:09.853138",
     "exception": false,
     "start_time": "2021-06-13T15:28:09.519689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_bow1_NB = c_report(y_valid,pred_bow1_NB)\n",
    "plot_cm(y_valid,pred_bow1_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-large",
   "metadata": {
    "papermill": {
     "duration": 0.235389,
     "end_time": "2021-06-13T15:28:10.348908",
     "exception": false,
     "start_time": "2021-06-13T15:28:10.113519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Creating a MultinomialNB model and training it with TF-IDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-valentine",
   "metadata": {
    "papermill": {
     "duration": 0.618329,
     "end_time": "2021-06-13T15:28:11.202178",
     "exception": false,
     "start_time": "2021-06-13T15:28:10.583849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tfidf1_NB = MultinomialNB(alpha=0.7)\n",
    "model_tfidf1_NB.fit(X_train_tfidf1,y_train)\n",
    "pred_tfidf1_NB = model_tfidf1_NB.predict(X_valid_tfidf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-breeding",
   "metadata": {
    "papermill": {
     "duration": 0.23629,
     "end_time": "2021-06-13T15:28:11.726308",
     "exception": false,
     "start_time": "2021-06-13T15:28:11.490018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Printing classification report and ploting confusion matrix for the predictions of MultinomialNB(TF-IDF,n-grams=1) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-kennedy",
   "metadata": {
    "papermill": {
     "duration": 0.346965,
     "end_time": "2021-06-13T15:28:12.311456",
     "exception": false,
     "start_time": "2021-06-13T15:28:11.964491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_tfidf1_NB = c_report(y_valid,pred_tfidf1_NB)\n",
    "plot_cm(y_valid,pred_tfidf1_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-hurricane",
   "metadata": {
    "papermill": {
     "duration": 0.236203,
     "end_time": "2021-06-13T15:28:12.786584",
     "exception": false,
     "start_time": "2021-06-13T15:28:12.550381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.3 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-exploration",
   "metadata": {
    "papermill": {
     "duration": 0.236736,
     "end_time": "2021-06-13T15:28:13.259250",
     "exception": false,
     "start_time": "2021-06-13T15:28:13.022514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### About Random Forest Classifier   \n",
    "\n",
    "Random forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.\n",
    "\n",
    "Random forests has a variety of applications, such as recommendation engines, image classification and feature selection. It can be used to classify loyal loan applicants, identify fraudulent activity and predict diseases. It lies at the base of the Boruta algorithm, which selects important features in a dataset.   \n",
    "\n",
    "Now let's create a [RandomForestClassifier](https://www.geeksforgeeks.org/random-forest-classifier-using-scikit-learn/) model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-spotlight",
   "metadata": {
    "papermill": {
     "duration": 85.628823,
     "end_time": "2021-06-13T15:29:39.125201",
     "exception": false,
     "start_time": "2021-06-13T15:28:13.496378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tfidf1_RFC = RandomForestClassifier()\n",
    "model_tfidf1_RFC.fit(X_train_tfidf1,y_train)\n",
    "pred_tfidf1_RFC = model_tfidf1_RFC.predict(X_valid_tfidf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-alloy",
   "metadata": {
    "papermill": {
     "duration": 0.237065,
     "end_time": "2021-06-13T15:29:39.600315",
     "exception": false,
     "start_time": "2021-06-13T15:29:39.363250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Printing classification report and ploting confusion matrix for predictions of RandomForestClassifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-wilderness",
   "metadata": {
    "papermill": {
     "duration": 0.344789,
     "end_time": "2021-06-13T15:29:40.185453",
     "exception": false,
     "start_time": "2021-06-13T15:29:39.840664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_tfidf1_RFC = c_report(y_valid,pred_tfidf1_RFC)\n",
    "plot_cm(y_valid,pred_tfidf1_RFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-banana",
   "metadata": {
    "papermill": {
     "duration": 0.238242,
     "end_time": "2021-06-13T15:29:40.663959",
     "exception": false,
     "start_time": "2021-06-13T15:29:40.425717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.4 eXtreme Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-timothy",
   "metadata": {
    "papermill": {
     "duration": 0.283763,
     "end_time": "2021-06-13T15:29:41.185856",
     "exception": false,
     "start_time": "2021-06-13T15:29:40.902093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### About XGBClassifier\n",
    "The XGBoost stands for eXtreme Gradient Boosting, which is a boosting algorithm based on gradient boosted decision trees algorithm.     \n",
    "XGBoost applies a better regularization technique to reduce overfitting, and it is one of the differences from the gradient boosting.    \n",
    "\n",
    "Now let's create a [XGBClassifier](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/) model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-drilling",
   "metadata": {
    "papermill": {
     "duration": 99.491415,
     "end_time": "2021-06-13T15:31:20.915423",
     "exception": false,
     "start_time": "2021-06-13T15:29:41.424008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tfidf1_XGB = XGBClassifier(eval_metric='mlogloss')\n",
    "model_tfidf1_XGB.fit(X_train_tfidf1,y_train)\n",
    "pred_tfidf1_XGB = model_tfidf1_XGB.predict(X_valid_tfidf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-opera",
   "metadata": {
    "papermill": {
     "duration": 0.237615,
     "end_time": "2021-06-13T15:31:21.390653",
     "exception": false,
     "start_time": "2021-06-13T15:31:21.153038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Printing classification report and ploting confusion matrix for the predictions made by the XGBClassifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-selling",
   "metadata": {
    "papermill": {
     "duration": 0.34054,
     "end_time": "2021-06-13T15:31:21.970276",
     "exception": false,
     "start_time": "2021-06-13T15:31:21.629736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_tfidf1_XGB = c_report(y_valid,pred_tfidf1_XGB)\n",
    "plot_cm(y_valid,pred_tfidf1_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-portfolio",
   "metadata": {
    "papermill": {
     "duration": 0.240372,
     "end_time": "2021-06-13T15:31:22.477933",
     "exception": false,
     "start_time": "2021-06-13T15:31:22.237561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.5 CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-spanking",
   "metadata": {
    "papermill": {
     "duration": 0.24115,
     "end_time": "2021-06-13T15:31:22.960130",
     "exception": false,
     "start_time": "2021-06-13T15:31:22.718980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### About CatBoostClassifier\n",
    "\n",
    "CatBoost is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Google’s TensorFlow and Apple’s Core ML. It can work with diverse data types to help solve a wide range of problems that businesses face today. To top it up, it provides best-in-class accuracy.    \n",
    "\n",
    "It yields state-of-the-art results without extensive data training typically required by other machine learning methods. \n",
    "   \n",
    "“Boost” comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library.      \n",
    "Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection, recommendation items, forecasting and it performs well also. It can also return very good result with relatively less data, unlike DL models that need to learn from a massive amount of data.\n",
    "\n",
    "\n",
    "Now let's create a [CatBoostClassifier](https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html) model with 100 iterations and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-filename",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 10.143559,
     "end_time": "2021-06-13T15:31:33.344042",
     "exception": false,
     "start_time": "2021-06-13T15:31:23.200483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tfidf1_CBC = CatBoostClassifier(iterations=100)\n",
    "model_tfidf1_CBC.fit(X_train_tfidf1,y_train)\n",
    "pred_tfidf1_CBC = model_tfidf1_CBC.predict(X_valid_tfidf1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-jungle",
   "metadata": {
    "papermill": {
     "duration": 0.248945,
     "end_time": "2021-06-13T15:31:33.843724",
     "exception": false,
     "start_time": "2021-06-13T15:31:33.594779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Printing classification report and ploting confusion matrix for the predictions made by the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-beaver",
   "metadata": {
    "papermill": {
     "duration": 0.408179,
     "end_time": "2021-06-13T15:31:34.535026",
     "exception": false,
     "start_time": "2021-06-13T15:31:34.126847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_tfidf1_CBC = c_report(y_valid,pred_tfidf1_CBC)\n",
    "plot_cm(y_valid,pred_tfidf1_CBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-millennium",
   "metadata": {
    "papermill": {
     "duration": 0.247809,
     "end_time": "2021-06-13T15:31:35.041010",
     "exception": false,
     "start_time": "2021-06-13T15:31:34.793201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.6 Support Vector CLassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-hormone",
   "metadata": {
    "papermill": {
     "duration": 0.248069,
     "end_time": "2021-06-13T15:31:35.539255",
     "exception": false,
     "start_time": "2021-06-13T15:31:35.291186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### About SVC\n",
    "SVM offers very high accuracy compared to other classifiers such as logistic regression, and decision trees. It is known for its kernel trick to handle nonlinear input spaces. It is used in a variety of applications such as face detection, intrusion detection, classification of emails, news articles and web pages, classification of genes, and handwriting recognition.\n",
    "\n",
    "SVM is an exciting algorithm and the concepts are relatively simple. The classifier separates data points using a hyperplane with the largest amount of margin. That's why an SVM classifier is also known as a discriminative classifier. SVM finds an optimal hyperplane which helps in classifying new data points.   \n",
    "\n",
    "Now let's create a [SVC](https://pythonprogramming.net/linear-svc-example-scikit-learn-svm-python/) model and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-creator",
   "metadata": {
    "papermill": {
     "duration": 204.642933,
     "end_time": "2021-06-13T15:35:00.432667",
     "exception": false,
     "start_time": "2021-06-13T15:31:35.789734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tfidf1_SVC = SVC(kernel='linear', degree=3, gamma='auto')\n",
    "model_tfidf1_SVC.fit(X_train_tfidf1,y_train)\n",
    "pred_tfidf1_SVC = model_tfidf1_SVC.predict(X_valid_tfidf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-bicycle",
   "metadata": {
    "papermill": {
     "duration": 0.249041,
     "end_time": "2021-06-13T15:35:00.984465",
     "exception": false,
     "start_time": "2021-06-13T15:35:00.735424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Printing classification report and ploting confusion matrix for the SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-saying",
   "metadata": {
    "papermill": {
     "duration": 0.388307,
     "end_time": "2021-06-13T15:35:01.668492",
     "exception": false,
     "start_time": "2021-06-13T15:35:01.280185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_tfidf1_SVC = c_report(y_valid,pred_tfidf1_SVC)\n",
    "plot_cm(y_valid,pred_tfidf1_SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-technology",
   "metadata": {
    "papermill": {
     "duration": 0.252538,
     "end_time": "2021-06-13T15:35:02.179289",
     "exception": false,
     "start_time": "2021-06-13T15:35:01.926751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.7 Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-services",
   "metadata": {
    "papermill": {
     "duration": 0.250215,
     "end_time": "2021-06-13T15:35:02.678059",
     "exception": false,
     "start_time": "2021-06-13T15:35:02.427844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### About Voting Classifier\n",
    "\n",
    "A Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on    \n",
    "their highest probability of chosen class as the output. It simply aggregates the findings of each classifier passed into Voting Classifier    \n",
    "and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n",
    "\n",
    "#### Voting Classifier supports two types of votings :  \n",
    "\n",
    "\n",
    "* **Hard Voting** : In hard voting, the predicted output class is a class with the highest majority of votes i.e the class    which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction.\n",
    "\n",
    "\n",
    "* **Soft Voting** : In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier\n",
    " \n",
    " \n",
    "Now let's create a [VotingClassifier](https://www.geeksforgeeks.org/ml-voting-classifier-using-sklearn/) with soft voting and train it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-agriculture",
   "metadata": {
    "papermill": {
     "duration": 104.463086,
     "end_time": "2021-06-13T15:36:47.391421",
     "exception": false,
     "start_time": "2021-06-13T15:35:02.928335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimators = []\n",
    "estimators.append(('LR', \n",
    "                  LogisticRegression()))\n",
    "estimators.append(('NB', MultinomialNB(alpha=0.7)))\n",
    "estimators.append(('XBG', XGBClassifier(eval_metric='mlogloss')))\n",
    "\n",
    "model_tfidf1_VC = VotingClassifier(estimators=estimators,voting='soft')\n",
    "model_tfidf1_VC.fit(X_train_tfidf1,y_train)\n",
    "pred_tfidf1_VC = model_tfidf1_VC.predict(X_valid_tfidf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-granny",
   "metadata": {
    "papermill": {
     "duration": 0.25035,
     "end_time": "2021-06-13T15:36:47.891056",
     "exception": false,
     "start_time": "2021-06-13T15:36:47.640706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Printing classification report and ploting confusion matrix for VotingClasssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-fancy",
   "metadata": {
    "papermill": {
     "duration": 0.416539,
     "end_time": "2021-06-13T15:36:48.558838",
     "exception": false,
     "start_time": "2021-06-13T15:36:48.142299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_tfidf1_VC = c_report(y_valid,pred_tfidf1_VC)\n",
    "plot_cm(y_valid,pred_tfidf1_VC,cmap = \"Greens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-desert",
   "metadata": {
    "papermill": {
     "duration": 0.250601,
     "end_time": "2021-06-13T15:36:49.059955",
     "exception": false,
     "start_time": "2021-06-13T15:36:48.809354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Comparing the Accuracy of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-strike",
   "metadata": {
    "papermill": {
     "duration": 0.307919,
     "end_time": "2021-06-13T15:36:49.621462",
     "exception": false,
     "start_time": "2021-06-13T15:36:49.313543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame([[\"Logistic Regression BoW1\",acc_bow1_logreg],[\"Logistic Regression BoW2\",acc_bow2_logreg],\n",
    "                       [\"Logistic Regression BoW3\",acc_bow3_logreg],[\"Logistic Regression Tf-Idf1\",acc_tfidf1_logreg],\n",
    "                       [\"Naive Bayes Tf-Idf1\",acc_tfidf1_NB],[\"Random Forest Tf-Idf1\",acc_tfidf1_RFC],\n",
    "                       [\"XGBClassifier Tf-Idf1\",acc_tfidf1_XGB],[\"CatBoost Tf-Idf1\",acc_tfidf1_CBC],\n",
    "                        [\"SVC Tf-Idf1\",acc_tfidf1_SVC],[\"Voting Tf-Idf1\",acc_tfidf1_VC]],\n",
    "                       columns = [\"Models\",\"Accuracy Score\"]).sort_values(by='Accuracy Score',ascending=False)\n",
    "\n",
    "results.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-culture",
   "metadata": {
    "papermill": {
     "duration": 0.252909,
     "end_time": "2021-06-13T15:36:50.128535",
     "exception": false,
     "start_time": "2021-06-13T15:36:49.875626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-investment",
   "metadata": {
    "papermill": {
     "duration": 0.25317,
     "end_time": "2021-06-13T15:36:50.635786",
     "exception": false,
     "start_time": "2021-06-13T15:36:50.382616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Among all Simple classification models used above Voting Classifier performed best with tf-idf and ngrams = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 801.61147,
   "end_time": "2021-06-13T15:39:28.701944",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-13T15:26:07.090474",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
