{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e33ef9-72ae-4759-8cf8-4c1309e97b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def preprocess_text_lemma_spacy(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmatized_words = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])\n",
    "    \n",
    "def preprocess_text(text):\n",
    "   \n",
    "    text = re.sub(r'bin laden', 'Binladen', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", 'http', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b(?<!breaking)news\\b|\\b(?<!breaking)\\w*news\\w*\\b', 'news', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(r\"D:\\elggak\\kaggle\\Tweet Disaster Competition\\nlp-getting-started\\train.csv\")\n",
    "test_df = pd.read_csv(r\"D:\\elggak\\kaggle\\Tweet Disaster Competition\\nlp-getting-started\\test.csv\")\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text_lemma_spacy)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text_lemma_spacy)\n",
    "\n",
    "train_df = pd.read_csv(r\"D:\\elggak\\kaggle\\Tweet Disaster Competition\\nlp-getting-started\\train.csv\")\n",
    "test_df = pd.read_csv(r\"D:\\elggak\\kaggle\\Tweet Disaster Competition\\nlp-getting-started\\test.csv\")\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text_lemma_spacy)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text_lemma_spacy)\n",
    "train_df['text'] = train_df['text'].apply(stemming)\n",
    "test_df['text'] = test_df['text'].apply(stemming)\n",
    "train_df_id = train_df['id']\n",
    "test_df_id = test_df['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fd003a-2b74-4254-b664-f9825254c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['text']\n",
    "y = train_df['target']\n",
    "X_test = test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5bcfb5-5874-4b4a-b42b-02c589f1e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def get_avg_word_vector(text):\n",
    "    words = text.split()\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in word2vec_model:\n",
    "            word_vectors.append(word2vec_model[word])\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)  # Return zero vector if no words found\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f251f0-682a-48bf-a97a-37c815e22fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_word_vectors(text):\n",
    "    words = text.split()\n",
    "    word_vectors = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in word2vec_model:\n",
    "            word_vectors.append(torch.tensor(word2vec_model[word]))\n",
    "        else:\n",
    "            word_vectors.append(torch.zeros(word2vec_model.vector_size))  # OOV words as zero vectors\n",
    "\n",
    "    return torch.stack(word_vectors) if word_vectors else torch.zeros(1, word2vec_model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf441bcb-2785-43ae-80de-0e374fec3db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(get_word_vectors)\n",
    "X_test = X_test.apply(get_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be20efd-481c-4a5c-bd4b-76a0731082c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a53fb9b6-5c01-4c65-9c11-0c1b9d65c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: list) -> tuple:    \n",
    "    length = [len(x) for x in batch]\n",
    "    X = pad_sequence(X,batch_first=True, padding_value=0)\n",
    "    return X, torch.tensor(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5257117a-d953-4945-b26a-8d4faffbc81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3128fdd4-4495-4aa1-993b-3c77d2c2a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "train_dataset = TweetDataset(X_train)\n",
    "val_dataset = TweetDataset(X_val)\n",
    "test_dataset = TweetDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "227fcc9c-9c39-4e0b-b5b2-d80baf559ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dec9cb1-5750-4234-a6f1-54f33ef11336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_tensor = torch.tensor(np.vstack(X_train.to_numpy()), dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "# X_val_tensor = torch.tensor(np.vstack(X_val.to_numpy()), dtype=torch.float32)\n",
    "# y_val_tensor = torch.tensor(y_val.to_numpy(), dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(np.vstack(X_test.to_numpy()), dtype=torch.float32)\n",
    "\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "# test_dataset = TensorDataset(X_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZES, shuffle=True,collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZES,shuffle=True,collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZES, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "041ae281-38e0-4ad8-9c93-9afd3d5ccb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f66526-bf5d-4c9b-8b89-8e3aa808ea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17ccabdf-87e1-473d-9141-3514a5b4c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetRNNModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_layers, output_size):\n",
    "        super(TweetRNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "    \n",
    "        packed_x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_rnn_out, _ = self.rnn(packed_x)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_rnn_out, batch_first=True)\n",
    "        batch_size = rnn_out.size(0)\n",
    "        last_outputs = rnn_out[torch.arange(batch_size), lengths - 1]\n",
    "        out = self.fc(last_outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c044b8e-bd47-4283-9903-2e3efa6a01b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 300])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62113e73-1f98-4c9d-8452-36b14fdf015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_model = TweetRNNModel(300,16,2,1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(tweet_model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae6532f1-d496-4955-afa1-e1018df915a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(all_labels, all_preds):\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy_score(all_labels, all_preds)),\n",
    "        'confusion_matrix': confusion_matrix(all_labels, all_preds),  # It's fine to leave the matrix as-is\n",
    "        'precision': float(precision_score(all_labels, all_preds)),\n",
    "        'recall': float(recall_score(all_labels, all_preds)),\n",
    "        'f1': float(f1_score(all_labels, all_preds)),\n",
    "        'macro_precision': float(precision_score(all_labels, all_preds, average='macro')),\n",
    "        'macro_recall': float(recall_score(all_labels, all_preds, average='macro')),\n",
    "        'macro_f1': float(f1_score(all_labels, all_preds, average='macro')),\n",
    "        'micro_precision': float(precision_score(all_labels, all_preds, average='micro')),\n",
    "        'micro_recall': float(recall_score(all_labels, all_preds, average='micro')),\n",
    "        'micro_f1': float(f1_score(all_labels, all_preds, average='micro'))\n",
    "    }\n",
    "    \n",
    "    return metrics, classification_report(all_labels, all_preds, target_names=['ham', 'spam'],digits = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d87f276d-0eb1-4a3f-baeb-00e283bbf8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mode(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, vocab_size: int):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X, y, lengths  in data_loader:\n",
    "       \n",
    "        output, _ = model(X,lengths)\n",
    "        loss = loss_fn(output.view(-1,vocab_size), y.view(-1))\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(data_loader)\n",
    "    return train_loss\n",
    "\n",
    "def test_mode(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer,  vocab_size: int):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y, lengths in data_loader:\n",
    "            output, _ = model(X,lengths)\n",
    "            output, hidden = model(X, lengths, hidden)\n",
    "            loss = loss_fn(output.view(-1,vocab_size), y.view(-1))\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "        test_loss /= len(data_loader)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e70939d-7a31-48ca-9832-4aff1544f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mode(model: torch.nn.Module,data_loader: torch.utils.data.DataLoader, loss_fn:torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch, (X,y) in enumerate(data_loader):\n",
    "        y_preds = model(X)\n",
    "        loss = loss_fn(y_preds, y.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.sigmoid(y_preds).round()  # Apply sigmoid and threshold at 0.5\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        # running_accuracy +=\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(data_loader.dataset)} samples\")\n",
    "    train_loss = running_loss/len(data_loader)\n",
    "    \n",
    "    return train_loss, calculate_metrics(all_labels,all_preds)\n",
    "\n",
    "def test_mode(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch, (X,y) in enumerate(data_loader):\n",
    "        y_preds = model(X)\n",
    "        loss = loss_fn(y_preds, y.unsqueeze(1))\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.sigmoid(y_preds).round()  # Apply sigmoid and threshold at 0.5\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        if batch % 400 == 0:\n",
    "                print(f\"Looked at {batch * len(X)}/{len(data_loader.dataset)} samples\")\n",
    "    test_loss = running_loss/len(data_loader)\n",
    "    \n",
    "    return test_loss, calculate_metrics(all_labels,all_preds)\n",
    "\n",
    "def predict_on_test_set(model: torch.nn.Module, test_loader: torch.utils.data.DataLoader):\n",
    "    model.eval()  \n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients during inference\n",
    "        for batch, X in enumerate(test_loader):\n",
    "        \n",
    "            y_preds = model(X[0])\n",
    "            preds = torch.sigmoid(y_preds).round()  \n",
    "            all_preds.extend(preds.detach().cpu().numpy()) \n",
    "\n",
    "    return all_preds \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52720347-f871-46af-87c1-bd006c946579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b698e154a2d2420d8612998eddee3db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3668",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 3668",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m train_loss, (train_metrics, train_classification_report)\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# print(f\"Train metrics: {train_metrics}\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m, in \u001b[0;36mtrain_mode\u001b[1;34m(model, data_loader, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m      7\u001b[0m     y_preds \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_preds, y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m, in \u001b[0;36mTweetDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 3668"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Start timer\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 30\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_loss, (train_metrics, train_classification_report)= train_mode(tweet_model, train_loader, criterion, optimizer)\n",
    "    print(f\"Train loss: {train_loss:.5f}\")\n",
    "    # print(f\"Train metrics: {train_metrics}\")\n",
    "    print(train_classification_report)\n",
    "    \n",
    "    # Test/Validate the model\n",
    "    test_loss, (test_metrics,test_classification_report) = test_mode(tweet_model, val_loader, criterion, optimizer)\n",
    "    print(f\"Test loss: {test_loss:.5f}\")\n",
    "    # print(f\"Test metrics: {test_metrics}\")\n",
    "    print(test_classification_report)\n",
    "\n",
    "    print(\"___________________________________\")\n",
    "    \n",
    "# End timer\n",
    "train_time_end_on_cpu = timer()\n",
    "\n",
    "total_train_time_model = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(tweet_model.parameters()).device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3f57a-37b7-4429-b35c-bf46aa34bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_on_test_set(tweet_model, test_loader)\n",
    "y_pred = [int(pred[0]) for pred in y_pred]\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_df_id,\n",
    "    'target': y_pred\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv(r'D:\\Kaggle\\disaster tweets\\embedding_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e99ea-91e3-4886-b32f-caf8d588ccd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575520d8-ea6f-4b11-b35c-853cc5889a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
