{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f51a534a-2a44-4954-961a-0c37db2a320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49de66fa-14e4-4bcc-847e-0d91a6088bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text_lemma_spacy(text):\n",
    "    doc = nlp(text.lower()) \n",
    "    lemmatized_words = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "    return ' '.join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8f4c59-fc50-47db-aa76-682a7109f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stemming(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbc66f2-6378-429b-971a-41317e77d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\train.csv\")\n",
    "test_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e28a093-b419-46eb-a352-960645ee1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}|(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},?\\s\\d{4})\\b'\n",
    "time_pattern = r'\\b((0?[1-9]|1[0-2]):[0-5]\\d\\s?(AM|PM)|([01]\\d|2[0-3]):[0-5]\\d(:[0-5]\\d)?)\\b'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'bin laden', 'Binladen', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", 'http', text, flags=re.MULTILINE)  \n",
    "    #text = re.sub(r'\\@\\w+|\\#','', text)  \n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b(?<!breaking)news\\b|\\b(?<!breaking)\\w*news\\w*\\b', 'news', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text\n",
    "\n",
    "# train_df['text'] = train_df['text'].apply(lambda x: re.sub(date_pattern, 'DATETIME', x))\n",
    "# train_df['text'] = train_df['text'].apply(lambda x: re.sub(time_pattern, 'DATETIME', x))\n",
    "# test_df['text'] = test_df['text'].apply(lambda x: re.sub(date_pattern, 'DATETIME', x))\n",
    "# test_df['text'] = test_df['text'].apply(lambda x: re.sub(time_pattern, 'DATETIME', x))\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text_lemma_spacy)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text_lemma_spacy)\n",
    "train_df['text'] = train_df['text'].apply(stemming)\n",
    "test_df['text'] = test_df['text'].apply(stemming)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb9bd0b-7f73-4775-8aa4-cf94a4b11baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_id = train_df['id']\n",
    "test_df_id = test_df['id']\n",
    "X = train_df['text']\n",
    "y = train_df['target']\n",
    "X_test = test_df['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b19f43-f813-4f25-acdf-450832f18caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093bbfab-96a4-4ed9-a915-58f490d5b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X).toarray() \n",
    "y = y.values\n",
    "X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZES, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZES,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3acbfa75-e8ab-4692-aa85-f40f1957778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c27c9155-e573-45c1-aa83-a32290e4a580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bed6db1b-5b32-4c7d-ae8c-7ba68c07906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62d60434-4645-4b9c-bcb9-de23ac42b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDisasterRNNModel(nn.Module):\n",
    "    def __init__(self,input_shape,hidden_units,out_shape):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_shape, hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, out_shape)\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = X.unsqueeze(1)\n",
    "        output, hidden = self.rnn(X)\n",
    "        return self.fc(hidden[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c292b425-92ab-4ce0-aeea-d5de5399b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_model = TweetDisasterRNNModel(X_train.shape[1],16,1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(tweet_model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "541c8af4-7b1e-4782-aba0-cb28892b4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(all_labels, all_preds):\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy_score(all_labels, all_preds)),\n",
    "        'confusion_matrix': confusion_matrix(all_labels, all_preds),  # It's fine to leave the matrix as-is\n",
    "        'precision': float(precision_score(all_labels, all_preds)),\n",
    "        'recall': float(recall_score(all_labels, all_preds)),\n",
    "        'f1': float(f1_score(all_labels, all_preds)),\n",
    "        'macro_precision': float(precision_score(all_labels, all_preds, average='macro')),\n",
    "        'macro_recall': float(recall_score(all_labels, all_preds, average='macro')),\n",
    "        'macro_f1': float(f1_score(all_labels, all_preds, average='macro')),\n",
    "        'micro_precision': float(precision_score(all_labels, all_preds, average='micro')),\n",
    "        'micro_recall': float(recall_score(all_labels, all_preds, average='micro')),\n",
    "        'micro_f1': float(f1_score(all_labels, all_preds, average='micro'))\n",
    "    }\n",
    "    \n",
    "    return metrics, classification_report(all_labels, all_preds, target_names=['ham', 'spam'],digits = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efc8c6e0-9fe0-4537-8269-83714078e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mode(model: torch.nn.Module,data_loader: torch.utils.data.DataLoader, loss_fn:torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch, (X,y) in enumerate(data_loader):\n",
    "        y_preds = model(X)\n",
    "        loss = loss_fn(y_preds, y.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.sigmoid(y_preds).round()  # Apply sigmoid and threshold at 0.5\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        # running_accuracy +=\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(data_loader.dataset)} samples\")\n",
    "    train_loss = running_loss/len(data_loader)\n",
    "    \n",
    "    return train_loss, calculate_metrics(all_labels,all_preds)\n",
    "\n",
    "def test_mode(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch, (X,y) in enumerate(data_loader):\n",
    "        y_preds = model(X)\n",
    "        loss = loss_fn(y_preds, y.unsqueeze(1))\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.sigmoid(y_preds).round()  # Apply sigmoid and threshold at 0.5\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        if batch % 400 == 0:\n",
    "                print(f\"Looked at {batch * len(X)}/{len(data_loader.dataset)} samples\")\n",
    "    test_loss = running_loss/len(data_loader)\n",
    "    \n",
    "    return test_loss, calculate_metrics(all_labels,all_preds)\n",
    "\n",
    "def predict_on_test_set(model: torch.nn.Module, test_loader: torch.utils.data.DataLoader):\n",
    "    model.eval()  \n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients during inference\n",
    "        for batch, X in enumerate(test_loader):\n",
    "        \n",
    "            y_preds = model(X[0])\n",
    "            preds = torch.sigmoid(y_preds).round()  \n",
    "            all_preds.extend(preds.detach().cpu().numpy()) \n",
    "\n",
    "    return all_preds \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fd47258-414b-44af-85ac-c5405c58a385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf33f29bcdc445f2be5cab2b9dff5f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.67596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.572093  0.993080  0.725970      3468\n",
      "        spam   0.657143  0.017544  0.034175      2622\n",
      "\n",
      "    accuracy                       0.573071      6090\n",
      "   macro avg   0.614618  0.505312  0.380072      6090\n",
      "weighted avg   0.608711  0.573071  0.428123      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.66520\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.575379  1.000000  0.730464       874\n",
      "        spam   1.000000  0.006163  0.012251       649\n",
      "\n",
      "    accuracy                       0.576494      1523\n",
      "   macro avg   0.787689  0.503082  0.371358      1523\n",
      "weighted avg   0.756324  0.576494  0.424410      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 1\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.64520\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.601249  0.999135  0.750731      3468\n",
      "        spam   0.990826  0.123570  0.219736      2622\n",
      "\n",
      "    accuracy                       0.622167      6090\n",
      "   macro avg   0.796038  0.561352  0.485233      6090\n",
      "weighted avg   0.768978  0.622167  0.522115      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.63678\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.621873  0.995423  0.765508       874\n",
      "        spam   0.967742  0.184900  0.310479       649\n",
      "\n",
      "    accuracy                       0.650033      1523\n",
      "   macro avg   0.794807  0.590162  0.537993      1523\n",
      "weighted avg   0.769259  0.650033  0.571605      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 2\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.59956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.666087  0.993945  0.797640      3468\n",
      "        spam   0.977049  0.340961  0.505513      2622\n",
      "\n",
      "    accuracy                       0.712808      6090\n",
      "   macro avg   0.821568  0.667453  0.651576      6090\n",
      "weighted avg   0.799969  0.712808  0.671867      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.59698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.702725  0.973684  0.816307       874\n",
      "        spam   0.926282  0.445300  0.601457       649\n",
      "\n",
      "    accuracy                       0.748523      1523\n",
      "   macro avg   0.814504  0.709492  0.708882      1523\n",
      "weighted avg   0.797990  0.748523  0.724752      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 3\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.54108\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.757072  0.972318  0.851300      3468\n",
      "        spam   0.941320  0.587338  0.723344      2622\n",
      "\n",
      "    accuracy                       0.806568      6090\n",
      "   macro avg   0.849196  0.779828  0.787322      6090\n",
      "weighted avg   0.836399  0.806568  0.796210      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.55534\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.761816  0.940503  0.841782       874\n",
      "        spam   0.882883  0.604006  0.717292       649\n",
      "\n",
      "    accuracy                       0.797111      1523\n",
      "   macro avg   0.822350  0.772255  0.779537      1523\n",
      "weighted avg   0.813407  0.797111  0.788733      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 4\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.48268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.793753  0.959919  0.868964      3468\n",
      "        spam   0.926688  0.670099  0.777778      2622\n",
      "\n",
      "    accuracy                       0.835140      6090\n",
      "   macro avg   0.860220  0.815009  0.823371      6090\n",
      "weighted avg   0.850987  0.835140  0.829704      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.52027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.779195  0.908467  0.838880       874\n",
      "        spam   0.841270  0.653313  0.735473       649\n",
      "\n",
      "    accuracy                       0.799737      1523\n",
      "   macro avg   0.810233  0.780890  0.787176      1523\n",
      "weighted avg   0.805647  0.799737  0.794815      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 5\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.43184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.826229  0.950115  0.883852      3468\n",
      "        spam   0.917697  0.735698  0.816681      2622\n",
      "\n",
      "    accuracy                       0.857800      6090\n",
      "   macro avg   0.871963  0.842907  0.850266      6090\n",
      "weighted avg   0.865610  0.857800  0.854932      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.49532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.783890  0.913043  0.843552       874\n",
      "        spam   0.849505  0.661017  0.743501       649\n",
      "\n",
      "    accuracy                       0.805647      1523\n",
      "   macro avg   0.816697  0.787030  0.793526      1523\n",
      "weighted avg   0.811851  0.805647  0.800917      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 6\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.38907\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.845266  0.949827  0.894501      3468\n",
      "        spam   0.920657  0.770023  0.838629      2622\n",
      "\n",
      "    accuracy                       0.872414      6090\n",
      "   macro avg   0.882961  0.859925  0.866565      6090\n",
      "weighted avg   0.877725  0.872414  0.870446      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.47784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.795339  0.898169  0.843632       874\n",
      "        spam   0.833955  0.688752  0.754430       649\n",
      "\n",
      "    accuracy                       0.808930      1523\n",
      "   macro avg   0.814647  0.793461  0.799031      1523\n",
      "weighted avg   0.811795  0.808930  0.805621      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 7\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.35542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.863196  0.951557  0.905226      3468\n",
      "        spam   0.925893  0.800534  0.858662      2622\n",
      "\n",
      "    accuracy                       0.886535      6090\n",
      "   macro avg   0.894545  0.876046  0.881944      6090\n",
      "weighted avg   0.890190  0.886535  0.885178      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.46739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.793667  0.889016  0.838640       874\n",
      "        spam   0.821691  0.688752  0.749371       649\n",
      "\n",
      "    accuracy                       0.803677      1523\n",
      "   macro avg   0.807679  0.788884  0.794006      1523\n",
      "weighted avg   0.805609  0.803677  0.800600      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 8\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.32551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.875596  0.953864  0.913055      3468\n",
      "        spam   0.930796  0.820748  0.872315      2622\n",
      "\n",
      "    accuracy                       0.896552      6090\n",
      "   macro avg   0.903196  0.887306  0.892685      6090\n",
      "weighted avg   0.899362  0.896552  0.895515      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.45995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.798146  0.886728  0.840108       874\n",
      "        spam   0.820652  0.697997  0.754371       649\n",
      "\n",
      "    accuracy                       0.806303      1523\n",
      "   macro avg   0.809399  0.792362  0.797240      1523\n",
      "weighted avg   0.807737  0.806303  0.803573      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 9\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.30073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.885220  0.958478  0.920393      3468\n",
      "        spam   0.938330  0.835622  0.884002      2622\n",
      "\n",
      "    accuracy                       0.905583      6090\n",
      "   macro avg   0.911775  0.897050  0.902198      6090\n",
      "weighted avg   0.908086  0.905583  0.904725      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.45743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.803838  0.862700  0.832230       874\n",
      "        spam   0.794872  0.716487  0.753647       649\n",
      "\n",
      "    accuracy                       0.800394      1523\n",
      "   macro avg   0.799355  0.789594  0.792938      1523\n",
      "weighted avg   0.800017  0.800394  0.798743      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 10\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.27960\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.896570  0.957324  0.925952      3468\n",
      "        spam   0.937997  0.853928  0.893991      2622\n",
      "\n",
      "    accuracy                       0.912808      6090\n",
      "   macro avg   0.917284  0.905626  0.909971      6090\n",
      "weighted avg   0.914406  0.912808  0.912191      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.45492\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.798137  0.882151  0.838043       874\n",
      "        spam   0.815081  0.699538  0.752902       649\n",
      "\n",
      "    accuracy                       0.804334      1523\n",
      "   macro avg   0.806609  0.790844  0.795473      1523\n",
      "weighted avg   0.805357  0.804334  0.801762      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 11\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.26148\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.903601  0.962226  0.931993      3468\n",
      "        spam   0.945348  0.864226  0.902969      2622\n",
      "\n",
      "    accuracy                       0.920033      6090\n",
      "   macro avg   0.924475  0.913226  0.917481      6090\n",
      "weighted avg   0.921575  0.920033  0.919497      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.45321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.798964  0.882151  0.838499       874\n",
      "        spam   0.815412  0.701079  0.753935       649\n",
      "\n",
      "    accuracy                       0.804990      1523\n",
      "   macro avg   0.807188  0.791615  0.796217      1523\n",
      "weighted avg   0.805973  0.804990  0.802464      1523\n",
      "\n",
      "___________________________________\n",
      "Train time on cpu: 8.771 seconds\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Start timer\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 12\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_loss, (train_metrics, train_classification_report)= train_mode(tweet_model, train_loader, criterion, optimizer)\n",
    "    print(f\"Train loss: {train_loss:.5f}\")\n",
    "    # print(f\"Train metrics: {train_metrics}\")\n",
    "    print(train_classification_report)\n",
    "    \n",
    "    # Test/Validate the model\n",
    "    test_loss, (test_metrics,test_classification_report) = test_mode(tweet_model, val_loader, criterion, optimizer)\n",
    "    print(f\"Test loss: {test_loss:.5f}\")\n",
    "    # print(f\"Test metrics: {test_metrics}\")\n",
    "    print(test_classification_report)\n",
    "\n",
    "    print(\"___________________________________\")\n",
    "    \n",
    "# End timer\n",
    "train_time_end_on_cpu = timer()\n",
    "\n",
    "total_train_time_model = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(tweet_model.parameters()).device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36dcb485-a503-479a-a390-0da859431eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_on_test_set(tweet_model, test_loader)\n",
    "y_pred = [int(pred[0]) for pred in y_pred]\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_df_id,\n",
    "    'target': y_pred\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv(r'D:\\Kaggle\\disaster tweets\\simple_rnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506e038-44db-4a70-8e3a-bcd82cb3014b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
