{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9662f7f0-edb4-4c42-a807-1d65a5cb1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7132823b-45ea-4e6c-aedc-6e6491308c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to the first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess_text_lemma(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e71315f-b8ff-4bb0-b7bd-38796dcd0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3830d1f4-bc61-452a-96af-dd5bf0675e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\train.csv\")\n",
    "test_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f5aa53-30d2-4666-bd22-195fc4b286d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harish-4072\\AppData\\Local\\Temp\\ipykernel_31272\\3642746277.py:31: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  train_df['contains_country'] = train_df['text'].str.contains(r'\\b(israel|afghan|iran|iraq|lebanon|yemen|palestine)\\b', regex=True, case=False)\n",
      "C:\\Users\\harish-4072\\AppData\\Local\\Temp\\ipykernel_31272\\3642746277.py:32: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  test_df['contains_country'] = train_df['text'].str.contains(r'\\b(israel|afghan|iran|iraq|lebanon|yemen|palestine)\\b', regex=True, case=False)\n"
     ]
    }
   ],
   "source": [
    "date_pattern = r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}|(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},?\\s\\d{4})\\b'\n",
    "time_pattern = r'\\b((0?[1-9]|1[0-2]):[0-5]\\d\\s?(AM|PM)|([01]\\d|2[0-3]):[0-5]\\d(:[0-5]\\d)?)\\b'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'bin laden', 'Binladen', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", 'http', text, flags=re.MULTILINE)  \n",
    "    #text = re.sub(r'\\@\\w+|\\#','', text)  \n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b(?<!breaking)news\\b|\\b(?<!breaking)\\w*news\\w*\\b', 'news', text)\n",
    "    return text\n",
    "\n",
    "# train_df['text'] = train_df['location'].fillna('') + ' ' + train_df['text'].fillna('')\n",
    "# test_df['text'] = test_df['location'].fillna('') + ' ' + test_df['text'].fillna('')\n",
    "# train_df['text'] = train_df['keyword'].fillna('') + ' ' + train_df['text'].fillna('')\n",
    "# test_df['text'] = test_df['keyword'].fillna('') + ' ' + test_df['text'].fillna('')\n",
    "train_df['text'] = train_df['text'].apply(lambda x: re.sub(date_pattern, 'DATETIME', x))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: re.sub(time_pattern, 'DATETIME', x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: re.sub(date_pattern, 'DATETIME', x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: re.sub(time_pattern, 'DATETIME', x))\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text_lemma)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text_lemma)\n",
    "train_df['text'] = train_df['text'].apply(stemming)\n",
    "test_df['text'] = test_df['text'].apply(stemming)\n",
    "\n",
    "train_df['url'] = train_df['text'].str.contains(r'http|https', regex=True)\n",
    "test_df['url'] = test_df['text'].str.contains(r'http|https', regex=True)\n",
    "train_df['contains_country'] = train_df['text'].str.contains(r'\\b(israel|afghan|iran|iraq|lebanon|yemen|palestine)\\b', regex=True, case=False)\n",
    "test_df['contains_country'] = train_df['text'].str.contains(r'\\b(israel|afghan|iran|iraq|lebanon|yemen|palestine)\\b', regex=True, case=False)\n",
    "\n",
    "# train_df['isNews'] = train_df['text'].str.contains(r'news|News|Breakingnews|BreakingNews|breakingnews', regex=True)\n",
    "# test_df['isNews'] = test_df['text'].str.contains(r'news|News|Breakingnews|BreakingNews|breakingnews', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a544c5-f171-40b8-80db-771275ce6129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   id keyword location                                               text  \\\n",
       " 0   1     NaN      NaN  our deed be the reason of thi earthquak may al...   \n",
       " 1   4     NaN      NaN               forest fire near la rong sask canada   \n",
       " 2   5     NaN      NaN  all resid ask to shelter in place be be notifi...   \n",
       " 3   6     NaN      NaN     peopl receiv wildfir evacu order in california   \n",
       " 4   7     NaN      NaN  just get sent thi photo from rubi alaska a smo...   \n",
       " \n",
       "    target    url  contains_country  \n",
       " 0       1  False             False  \n",
       " 1       1  False             False  \n",
       " 2       1  False             False  \n",
       " 3       1  False             False  \n",
       " 4       1  False             False  ,\n",
       " 53291,\n",
       " 19578,\n",
       " id                     0\n",
       " keyword               61\n",
       " location            2533\n",
       " text                   0\n",
       " target                 0\n",
       " url                    0\n",
       " contains_country       0\n",
       " dtype: int64,\n",
       " id                     0\n",
       " keyword               26\n",
       " location            1105\n",
       " text                   0\n",
       " url                    0\n",
       " contains_country       0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(), train_df.size,test_df.size, train_df.isnull().sum(),test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd798dac-abe1-4e5f-ba24-503773486d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_id = train_df['id']\n",
    "test_df_id = test_df['id']\n",
    "X = train_df[['text','url','contains_country']]\n",
    "y = train_df['target']\n",
    "X_test = test_df[['text','url','contains_country']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "111353af-d889-485e-9050-e4f5abbf0499",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f7a201-1067-465e-bfbe-1f18fcc19899",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train['text'])\n",
    "X_val_vec = vectorizer.transform(X_val['text'])\n",
    "X_test_vec = vectorizer.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3821e7ae-d6e5-43c3-b65f-80cd2fcf0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_combined = hstack([X_train_vec,X_train[['url','contains_country']].values])\n",
    "X_val_combined = hstack([X_val_vec, X_val[['url','contains_country']].values])\n",
    "X_test_combined = hstack([X_test_vec, X_test[['url','contains_country']].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60de5628-ad1c-4147-bb69-807d8e7b3187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8069599474720945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.809829  0.867277  0.837569       874\n",
      "        spam   0.802385  0.725732  0.762136       649\n",
      "\n",
      "    accuracy                       0.806960      1523\n",
      "   macro avg   0.806107  0.796504  0.799852      1523\n",
      "weighted avg   0.806657  0.806960  0.805425      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_combined, y_train)\n",
    "y_pred = nb_model.predict(X_val_combined)\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred)}')\n",
    "print(classification_report(y_val, y_pred, target_names=['ham', 'spam'],digits = 6))\n",
    "y_pred = nb_model.predict(X_test_combined)\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_df_id,\n",
    "    'target': y_pred\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv(r'D:\\Kaggle\\disaster tweets\\nb_normal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea3875b8-fc86-4baf-a392-b5b26b410ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.799080761654629\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.797071  0.871854  0.832787       874\n",
      "        spam   0.802469  0.701079  0.748355       649\n",
      "\n",
      "    accuracy                       0.799081      1523\n",
      "   macro avg   0.799770  0.786466  0.790571      1523\n",
      "weighted avg   0.799371  0.799081  0.796808      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val_vec)\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred)}')\n",
    "print(classification_report(y_val, y_pred, target_names=['ham', 'spam'],digits = 6))\n",
    "y_pred = model.predict(vectorizer.transform(X_test))\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_df_id,\n",
    "    'target': y_pred\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv(r'D:\\Kaggle\\disaster tweets\\nb_logisticregression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364fd786-3926-4d2f-bf85-17a89b4ba9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7925147734734077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.794304  0.861556  0.826564       874\n",
      "        spam   0.789565  0.699538  0.741830       649\n",
      "\n",
      "    accuracy                       0.792515      1523\n",
      "   macro avg   0.791935  0.780547  0.784197      1523\n",
      "weighted avg   0.792285  0.792515  0.790456      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "\n",
    "nb_model = MultinomialNB(alpha=0.1)\n",
    "nb_model.fit(X_train_vec, y_train)\n",
    "y_pred = nb_model.predict(X_val_vec)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred)}')\n",
    "print(classification_report(y_val, y_pred, target_names=['ham', 'spam'],digits = 6))\n",
    "y_pred = nb_model.predict(vectorizer.transform(X_test))\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_df_id,\n",
    "    'target': y_pred\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv(r'D:\\Kaggle\\disaster tweets\\nb_tfid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b39f1c9c-e7f1-4146-a7dc-9105bc932fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8036769533814839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.800418  0.876430  0.836701       874\n",
      "        spam   0.809187  0.705701  0.753909       649\n",
      "\n",
      "    accuracy                       0.803677      1523\n",
      "   macro avg   0.804803  0.791066  0.795305      1523\n",
      "weighted avg   0.804155  0.803677  0.801421      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val_vec)\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred)}')\n",
    "print(classification_report(y_val, y_pred, target_names=['ham', 'spam'],digits = 6))\n",
    "y_pred = model.predict(vectorizer.transform(X_test))\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_df_id,\n",
    "    'target': y_pred\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv(r'D:\\Kaggle\\disaster tweets\\nb_logisticregression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c87b8-ef5a-4baa-8fcf-61ef4aaeb48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\train.csv\")\n",
    "test_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88af3c-9622-48a3-a7e4-e1f433c0e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['url'] = train_df['text'].str.contains(r'crushed|Crushed', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8921f6-473b-42a4-a277-dfafc03a1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['url'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d6c997-0a25-4d0c-a492-389969cc6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = train_df[train_df['url']]\n",
    "filtered_df['target'].value_counts().plot(kind='bar', color=['blue', 'orange'])\n",
    "\n",
    "plt.title('Distribution of Output for Texts Containing HTTP/HTTPS')\n",
    "plt.xlabel('Output')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0155a-5a67-4f93-8c35-da622b16eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "train_df[train_df['id']==9841]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b4da6-cd63-446a-bded-d4c61acfd5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Example time strings\n",
    "times = [\n",
    "    \"14:30\",\n",
    "    \"23:59:59\",\n",
    "    \"03:45 PM\",\n",
    "    \"11:59 AM\",\n",
    "    \"7:30\",\n",
    "    \"23:60\",  # Invalid time\n",
    "    \"00:00\"\n",
    "]\n",
    "\n",
    "# 24-hour time regex\n",
    "time_pattern_24 = r'\\b([01]\\d|2[0-3]):[0-5]\\d(:[0-5]\\d)?\\b'\n",
    "# 12-hour time regex\n",
    "time_pattern_12 = r'\\b(0?[1-9]|1[0-2]):[0-5]\\d(:[0-5]\\d)?\\s?(AM|PM)\\b'\n",
    "\n",
    "# Check for matches\n",
    "for time in times:\n",
    "    if re.match(time_pattern_24, time) or re.match(time_pattern_12, time):\n",
    "        print(f\"Valid time: {time}\")\n",
    "    else:\n",
    "        print(f\"Invalid time: {time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3241f1-6bc6-4845-a71a-10c398cece8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Example regex for common date formats\n",
    "date_pattern = r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}|(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},?\\s\\d{4})\\b'\n",
    "\n",
    "# Example regex for common time formats\n",
    "time_pattern = r'\\b((0?[1-9]|1[0-2]):[0-5]\\d\\s?(AM|PM)|([01]\\d|2[0-3]):[0-5]\\d(:[0-5]\\d)?)\\b'\n",
    "\n",
    "# Example DataFrame (replace this with your actual DataFrame)\n",
    "# train_df = pd.read_csv('path_to_your_tweets_disaster_dataset.csv')\n",
    "\n",
    "# Example text to replace times and dates\n",
    "train_df = pd.DataFrame({\n",
    "    'id': [9833],\n",
    "    'text': [\"The earthquake happened on 15-03-2015 at 04:30 AM Meeting at 03:45 PM on 04/22/2022.\"]\n",
    "})\n",
    "\n",
    "# Replace dates with 'DATE'\n",
    "train_df['text'] = train_df['text'].apply(lambda x: re.sub(date_pattern, 'DATE', x))\n",
    "\n",
    "# Replace times with 'TIME'\n",
    "train_df['text'] = train_df['text'].apply(lambda x: re.sub(time_pattern, 'TIME', x))\n",
    "\n",
    "# Display the result\n",
    "print(train_df['text'].iloc[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e4844-0f8c-4f82-a8a2-0327626e5f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
