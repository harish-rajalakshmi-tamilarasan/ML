{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f51a534a-2a44-4954-961a-0c37db2a320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49de66fa-14e4-4bcc-847e-0d91a6088bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to the first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess_text_lemma(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8f4c59-fc50-47db-aa76-682a7109f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbc66f2-6378-429b-971a-41317e77d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\train.csv\")\n",
    "test_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e28a093-b419-46eb-a352-960645ee1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}|(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},?\\s\\d{4})\\b'\n",
    "time_pattern = r'\\b((0?[1-9]|1[0-2]):[0-5]\\d\\s?(AM|PM)|([01]\\d|2[0-3]):[0-5]\\d(:[0-5]\\d)?)\\b'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'bin laden', 'Binladen', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", 'http', text, flags=re.MULTILINE)  \n",
    "    #text = re.sub(r'\\@\\w+|\\#','', text)  \n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b(?<!breaking)news\\b|\\b(?<!breaking)\\w*news\\w*\\b', 'news', text)\n",
    "    return text\n",
    "\n",
    "# train_df['text'] = train_df['location'].fillna('') + ' ' + train_df['text'].fillna('')\n",
    "# test_df['text'] = test_df['location'].fillna('') + ' ' + test_df['text'].fillna('')\n",
    "# train_df['text'] = train_df['keyword'].fillna('') + ' ' + train_df['text'].fillna('')\n",
    "# test_df['text'] = test_df['keyword'].fillna('') + ' ' + test_df['text'].fillna('')\n",
    "train_df['text'] = train_df['text'].apply(lambda x: re.sub(date_pattern, 'DATETIME', x))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: re.sub(time_pattern, 'DATETIME', x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: re.sub(date_pattern, 'DATETIME', x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: re.sub(time_pattern, 'DATETIME', x))\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text_lemma)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text_lemma)\n",
    "train_df['text'] = train_df['text'].apply(stemming)\n",
    "test_df['text'] = test_df['text'].apply(stemming)\n",
    "\n",
    "# train_df['url'] = train_df['text'].str.contains(r'http|https', regex=True)\n",
    "# test_df['url'] = test_df['text'].str.contains(r'http|https', regex=True)\n",
    "# train_df['contains_country'] = train_df['text'].str.contains(r'\\b(israel|afghan|iran|iraq|lebanon|yemen|palestine)\\b', regex=True, case=False)\n",
    "# test_df['contains_country'] = train_df['text'].str.contains(r'\\b(israel|afghan|iran|iraq|lebanon|yemen|palestine)\\b', regex=True, case=False)\n",
    "\n",
    "# train_df['isNews'] = train_df['text'].str.contains(r'news|News|Breakingnews|BreakingNews|breakingnews', regex=True)\n",
    "# test_df['isNews'] = test_df['text'].str.contains(r'news|News|Breakingnews|BreakingNews|breakingnews', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0501a7b1-6d02-448b-8de7-788a4935ec4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   id keyword location                                               text  \\\n",
       " 0   1     NaN      NaN  our deed be the reason of thi earthquak may al...   \n",
       " 1   4     NaN      NaN               forest fire near la rong sask canada   \n",
       " 2   5     NaN      NaN  all resid ask to shelter in place be be notifi...   \n",
       " 3   6     NaN      NaN     peopl receiv wildfir evacu order in california   \n",
       " 4   7     NaN      NaN  just get sent thi photo from rubi alaska a smo...   \n",
       " \n",
       "    target  \n",
       " 0       1  \n",
       " 1       1  \n",
       " 2       1  \n",
       " 3       1  \n",
       " 4       1  ,\n",
       " 38065,\n",
       " 13052,\n",
       " id             0\n",
       " keyword       61\n",
       " location    2533\n",
       " text           0\n",
       " target         0\n",
       " dtype: int64,\n",
       " id             0\n",
       " keyword       26\n",
       " location    1105\n",
       " text           0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(), train_df.size,test_df.size, train_df.isnull().sum(),test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb9bd0b-7f73-4775-8aa4-cf94a4b11baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_id = train_df['id']\n",
    "test_df_id = test_df['id']\n",
    "X = train_df['text']\n",
    "y = train_df['target']\n",
    "X_test = test_df['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c61f7a7-4e79-4962-a7cd-d4284813351f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         just happen a terribl car crash\n",
       "1       heard about earthquak be differ citi stay safe...\n",
       "2       there be a forest fire at spot pond gees be fl...\n",
       "3                          apocalyps light spokan wildfir\n",
       "4               typhoon soudelor kill in china and taiwan\n",
       "                              ...                        \n",
       "3258      earthquak safeti lo angel ûò safeti fasten xrwn\n",
       "3259    storm in ri bad than last hurrican my cityampo...\n",
       "3260                    green line derail in chicago http\n",
       "3261             meg issu hazard weather outlook hwo http\n",
       "3262    cityofcalgari have activ it municip emerg plan...\n",
       "Name: text, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b19f43-f813-4f25-acdf-450832f18caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "093bbfab-96a4-4ed9-a915-58f490d5b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X).toarray() \n",
    "y = y.values\n",
    "X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZES, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZES,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3acbfa75-e8ab-4692-aa85-f40f1957778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c27c9155-e573-45c1-aa83-a32290e4a580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bed6db1b-5b32-4c7d-ae8c-7ba68c07906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62d60434-4645-4b9c-bcb9-de23ac42b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDisasterRNNModel(nn.Module):\n",
    "    def __init__(self,input_shape,hidden_units,out_shape):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_shape, hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, out_shape)\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = X.unsqueeze(1)\n",
    "        output, hidden = self.rnn(X)\n",
    "        return self.fc(hidden[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c292b425-92ab-4ce0-aeea-d5de5399b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_model = TweetDisasterRNNModel(X_train.shape[1],16,1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(tweet_model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "541c8af4-7b1e-4782-aba0-cb28892b4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(all_labels, all_preds):\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy_score(all_labels, all_preds)),\n",
    "        'confusion_matrix': confusion_matrix(all_labels, all_preds),  # It's fine to leave the matrix as-is\n",
    "        'precision': float(precision_score(all_labels, all_preds)),\n",
    "        'recall': float(recall_score(all_labels, all_preds)),\n",
    "        'f1': float(f1_score(all_labels, all_preds)),\n",
    "        'macro_precision': float(precision_score(all_labels, all_preds, average='macro')),\n",
    "        'macro_recall': float(recall_score(all_labels, all_preds, average='macro')),\n",
    "        'macro_f1': float(f1_score(all_labels, all_preds, average='macro')),\n",
    "        'micro_precision': float(precision_score(all_labels, all_preds, average='micro')),\n",
    "        'micro_recall': float(recall_score(all_labels, all_preds, average='micro')),\n",
    "        'micro_f1': float(f1_score(all_labels, all_preds, average='micro'))\n",
    "    }\n",
    "    \n",
    "    return metrics, classification_report(all_labels, all_preds, target_names=['ham', 'spam'],digits = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efc8c6e0-9fe0-4537-8269-83714078e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mode(model: torch.nn.Module,data_loader: torch.utils.data.DataLoader, loss_fn:torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch, (X,y) in enumerate(data_loader):\n",
    "        y_preds = model(X)\n",
    "        loss = loss_fn(y_preds, y.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.sigmoid(y_preds).round()  # Apply sigmoid and threshold at 0.5\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        # running_accuracy +=\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(data_loader.dataset)} samples\")\n",
    "    train_loss = running_loss/len(data_loader)\n",
    "    \n",
    "    return train_loss, calculate_metrics(all_labels,all_preds)\n",
    "\n",
    "def test_mode(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch, (X,y) in enumerate(data_loader):\n",
    "        y_preds = model(X)\n",
    "        loss = loss_fn(y_preds, y.unsqueeze(1))\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.sigmoid(y_preds).round()  # Apply sigmoid and threshold at 0.5\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        if batch % 400 == 0:\n",
    "                print(f\"Looked at {batch * len(X)}/{len(data_loader.dataset)} samples\")\n",
    "    test_loss = running_loss/len(data_loader)\n",
    "    \n",
    "    return test_loss, calculate_metrics(all_labels,all_preds)\n",
    "\n",
    "def predict_on_test_set(model: torch.nn.Module, test_loader: torch.utils.data.DataLoader):\n",
    "    model.eval()  \n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients during inference\n",
    "        for batch, X in enumerate(test_loader):\n",
    "        \n",
    "            y_preds = model(X[0])\n",
    "            preds = torch.sigmoid(y_preds).round()  \n",
    "            all_preds.extend(preds.detach().cpu().numpy()) \n",
    "\n",
    "    return all_preds \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fd47258-414b-44af-85ac-c5405c58a385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c51329dcd849c2a64c931ac5f52a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.44030\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.825599  0.954152  0.885233      3468\n",
      "        spam   0.923631  0.733410  0.817602      2622\n",
      "\n",
      "    accuracy                       0.859113      6090\n",
      "   macro avg   0.874615  0.843781  0.851417      6090\n",
      "weighted avg   0.867806  0.859113  0.856115      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.49518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.780849  0.905034  0.838368       874\n",
      "        spam   0.837255  0.657935  0.736842       649\n",
      "\n",
      "    accuracy                       0.799737      1523\n",
      "   macro avg   0.809052  0.781485  0.787605      1523\n",
      "weighted avg   0.804885  0.799737  0.795104      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 1\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.39693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.847938  0.948674  0.895482      3468\n",
      "        spam   0.919457  0.774981  0.841060      2622\n",
      "\n",
      "    accuracy                       0.873892      6090\n",
      "   macro avg   0.883698  0.861827  0.868271      6090\n",
      "weighted avg   0.878730  0.873892  0.872051      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.47677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.779528  0.906178  0.838095       874\n",
      "        spam   0.838264  0.654854  0.735294       649\n",
      "\n",
      "    accuracy                       0.799081      1523\n",
      "   macro avg   0.808896  0.780516  0.786695      1523\n",
      "weighted avg   0.804557  0.799081  0.794288      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 2\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.36139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.855963  0.956171  0.903296      3468\n",
      "        spam   0.931408  0.787185  0.853245      2622\n",
      "\n",
      "    accuracy                       0.883415      6090\n",
      "   macro avg   0.893685  0.871678  0.878271      6090\n",
      "weighted avg   0.888445  0.883415  0.881747      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.46455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.796087  0.884439  0.837940       874\n",
      "        spam   0.817029  0.694915  0.751041       649\n",
      "\n",
      "    accuracy                       0.803677      1523\n",
      "   macro avg   0.806558  0.789677  0.794491      1523\n",
      "weighted avg   0.805011  0.803677  0.800910      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 3\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.33121\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.872268  0.955017  0.911769      3468\n",
      "        spam   0.931967  0.815027  0.869583      2622\n",
      "\n",
      "    accuracy                       0.894745      6090\n",
      "   macro avg   0.902117  0.885022  0.890676      6090\n",
      "weighted avg   0.897971  0.894745  0.893606      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.45716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.796296  0.885584  0.838570       874\n",
      "        spam   0.818512  0.694915  0.751667       649\n",
      "\n",
      "    accuracy                       0.804334      1523\n",
      "   macro avg   0.807404  0.790249  0.795118      1523\n",
      "weighted avg   0.805763  0.804334  0.801538      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 4\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.30632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.881698  0.958478  0.918486      3468\n",
      "        spam   0.937931  0.829901  0.880615      2622\n",
      "\n",
      "    accuracy                       0.903120      6090\n",
      "   macro avg   0.909814  0.894189  0.899550      6090\n",
      "weighted avg   0.905908  0.903120  0.902181      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.45198\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.802105  0.871854  0.835526       874\n",
      "        spam   0.804538  0.710324  0.754501       649\n",
      "\n",
      "    accuracy                       0.803020      1523\n",
      "   macro avg   0.803321  0.791089  0.795014      1523\n",
      "weighted avg   0.803142  0.803020  0.800999      1523\n",
      "\n",
      "___________________________________\n",
      "Train time on cpu: 3.640 seconds\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Start timer\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 5\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_loss, (train_metrics, train_classification_report)= train_mode(tweet_model, train_loader, criterion, optimizer)\n",
    "    print(f\"Train loss: {train_loss:.5f}\")\n",
    "    # print(f\"Train metrics: {train_metrics}\")\n",
    "    print(train_classification_report)\n",
    "    \n",
    "    # Test/Validate the model\n",
    "    test_loss, (test_metrics,test_classification_report) = test_mode(tweet_model, val_loader, criterion, optimizer)\n",
    "    print(f\"Test loss: {test_loss:.5f}\")\n",
    "    # print(f\"Test metrics: {test_metrics}\")\n",
    "    print(test_classification_report)\n",
    "\n",
    "    print(\"___________________________________\")\n",
    "    \n",
    "# End timer\n",
    "train_time_end_on_cpu = timer()\n",
    "\n",
    "# Calculate total training time\n",
    "total_train_time_model = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(tweet_model.parameters()).device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36dcb485-a503-479a-a390-0da859431eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_on_test_set(tweet_model, test_loader)\n",
    "y_pred = [int(pred[0]) for pred in y_pred]\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_df_id,\n",
    "    'target': y_pred\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv(r'D:\\Kaggle\\disaster tweets\\simple_rnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506e038-44db-4a70-8e3a-bcd82cb3014b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
