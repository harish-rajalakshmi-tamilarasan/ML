{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c934f755-4c97-4fec-83d8-61d6725b7a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9989287-44c8-42fd-bf43-c1d88316eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to the first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess_text_lemma(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92699fba-46bb-43d3-ac21-a13f3ddd680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5482aa3d-fa14-4e12-8dd3-d1578c74ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\train.csv\")\n",
    "test_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc33e96-0f39-4ece-97a4-ca3f85dc7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}|(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},?\\s\\d{4})\\b'\n",
    "time_pattern = r'\\b((0?[1-9]|1[0-2]):[0-5]\\d\\s?(AM|PM)|([01]\\d|2[0-3]):[0-5]\\d(:[0-5]\\d)?)\\b'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'bin laden', 'Binladen', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", 'http', text, flags=re.MULTILINE)  \n",
    "    #text = re.sub(r'\\@\\w+|\\#','', text)  \n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b(?<!breaking)news\\b|\\b(?<!breaking)\\w*news\\w*\\b', 'news', text)\n",
    "    return text\n",
    "\n",
    "# train_df['text'] = train_df['location'].fillna('') + ' ' + train_df['text'].fillna('')\n",
    "# test_df['text'] = test_df['location'].fillna('') + ' ' + test_df['text'].fillna('')\n",
    "# train_df['text'] = train_df['keyword'].fillna('') + ' ' + train_df['text'].fillna('')\n",
    "# test_df['text'] = test_df['keyword'].fillna('') + ' ' + test_df['text'].fillna('')\n",
    "train_df['text'] = train_df['text'].apply(lambda x: re.sub(date_pattern, 'DATETIME', x))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: re.sub(time_pattern, 'DATETIME', x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: re.sub(date_pattern, 'DATETIME', x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: re.sub(time_pattern, 'DATETIME', x))\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text_lemma)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text_lemma)\n",
    "train_df['text'] = train_df['text'].apply(stemming)\n",
    "test_df['text'] = test_df['text'].apply(stemming)\n",
    "\n",
    "# train_df['url'] = train_df['text'].str.contains(r'http|https', regex=True)\n",
    "# test_df['url'] = test_df['text'].str.contains(r'http|https', regex=True)\n",
    "# train_df['contains_country'] = train_df['text'].str.contains(r'\\b(israel|afghan|iran|iraq|lebanon|yemen|palestine)\\b', regex=True, case=False)\n",
    "# test_df['contains_country'] = train_df['text'].str.contains(r'\\b(israel|afghan|iran|iraq|lebanon|yemen|palestine)\\b', regex=True, case=False)\n",
    "\n",
    "# train_df['isNews'] = train_df['text'].str.contains(r'news|News|Breakingnews|BreakingNews|breakingnews', regex=True)\n",
    "# test_df['isNews'] = test_df['text'].str.contains(r'news|News|Breakingnews|BreakingNews|breakingnews', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4137450-34b0-47ca-8aad-48e45ffacc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   id keyword location                                               text  \\\n",
       " 0   1     NaN      NaN  our deed be the reason of thi earthquak may al...   \n",
       " 1   4     NaN      NaN               forest fire near la rong sask canada   \n",
       " 2   5     NaN      NaN  all resid ask to shelter in place be be notifi...   \n",
       " 3   6     NaN      NaN     peopl receiv wildfir evacu order in california   \n",
       " 4   7     NaN      NaN  just get sent thi photo from rubi alaska a smo...   \n",
       " \n",
       "    target  \n",
       " 0       1  \n",
       " 1       1  \n",
       " 2       1  \n",
       " 3       1  \n",
       " 4       1  ,\n",
       " 38065,\n",
       " 13052,\n",
       " id             0\n",
       " keyword       61\n",
       " location    2533\n",
       " text           0\n",
       " target         0\n",
       " dtype: int64,\n",
       " id             0\n",
       " keyword       26\n",
       " location    1105\n",
       " text           0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(), train_df.size,test_df.size, train_df.isnull().sum(),test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509d1cf3-f4a7-45c4-8df4-50d490285f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_id = train_df['id']\n",
    "test_df_id = test_df['id']\n",
    "X = train_df['text']\n",
    "y = train_df['target']\n",
    "X_test = test_df['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff724b2c-7699-4c02-92ed-3a89e22ea8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(X).toarray()  # Convert to array for PyTorch\n",
    "y = y.values\n",
    "X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128,shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566caa46-c61b-49e2-bd91-f7a82ed7b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32909bfc-a8b9-4d1e-beec-c23b62da6dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe130837-6768-4fa2-8d0a-b96d567f2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6203f3d1-1ab4-4005-8fe3-0c0d3213250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDisasterModel(nn.Module):\n",
    "    def __init__(self,input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape,out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units,out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.layer_stack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb836c1-69a8-4ae7-b70d-ce2640196010",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TweetDisasterModel(X_train.shape[1],64,1)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1dc3471-fd9e-4f40-bcd3-e77551a6d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_pred,y_true):\n",
    "    correct = torch.eq(y_true, (torch.sigmoid(y_pred)>=0.5).float()).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab2d4d6b-6b2e-4b62-871a-fcb72c01ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def train_mode(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, accuracy_fn, optimizer: torch.optim.Optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    for batch, (X, y) in enumerate(data_loader):                \n",
    "        y_pred = model(X)\n",
    "        # loss = loss_fn(y,y_pred.squeeze())\n",
    "        # train_loss += loss.item()\n",
    "        # train_accuracy += accuracy_fn(y, y_pred.squeeze())\n",
    "        loss = loss_fn(y_pred.squeeze(), y)\n",
    "        train_loss += loss.item()\n",
    "        train_accuracy += accuracy_fn(y_pred.squeeze(), y)\n",
    "        #print(y_pred.shape,y_pred,y,train_accuracy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(data_loader.dataset)} samples\")\n",
    "    train_loss /= len(data_loader)\n",
    "    train_accuracy /= len(data_loader)\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def test_mode(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module, accuracy_fn, optimizer: torch.optim.Optimizer):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(data_loader):\n",
    "            test_pred = model(X)\n",
    "            test_loss += loss_fn(test_pred.squeeze(), y).item()\n",
    "            test_accuracy += accuracy_fn(test_pred.squeeze(), y)\n",
    "            # test_pred = model(X)\n",
    "            # test_loss += loss_fn(y, test_pred.squeeze()).item()\n",
    "            # test_accuracy += accuracy_fn(y, test_pred.squeeze())\n",
    "            if batch % 400 == 0:\n",
    "                print(f\"Looked at {batch * len(X)}/{len(data_loader.dataset)} samples\")\n",
    "        test_loss /= len(data_loader)\n",
    "        test_accuracy /= len(data_loader)\n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b75282c5-63a3-4e04-bea7-c06290d328a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1305f2fede4c9da55ce3541815d47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68659 | Train accuracy: 56.93227\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68541 | Train accuracy: 57.44565\n",
      "Epoch: 1\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68563 | Train accuracy: 57.05105\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68468 | Train accuracy: 57.40149\n",
      "Epoch: 2\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68512 | Train accuracy: 56.96791\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68401 | Train accuracy: 57.45301\n",
      "Epoch: 3\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68464 | Train accuracy: 56.96791\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68367 | Train accuracy: 57.37942\n",
      "Epoch: 4\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68433 | Train accuracy: 56.93227\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68330 | Train accuracy: 57.37942\n",
      "Epoch: 5\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68413 | Train accuracy: 56.88477\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68303 | Train accuracy: 57.37206\n",
      "Epoch: 6\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68368 | Train accuracy: 56.99166\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68289 | Train accuracy: 57.32054\n",
      "Epoch: 7\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68361 | Train accuracy: 56.93227\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68276 | Train accuracy: 57.29110\n",
      "Epoch: 8\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68332 | Train accuracy: 56.99166\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68232 | Train accuracy: 57.40885\n",
      "Epoch: 9\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68332 | Train accuracy: 56.93227\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68225 | Train accuracy: 57.37942\n",
      "Epoch: 10\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68306 | Train accuracy: 56.99166\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68216 | Train accuracy: 57.36470\n",
      "Epoch: 11\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68308 | Train accuracy: 56.94415\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68186 | Train accuracy: 57.44565\n",
      "Epoch: 12\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68280 | Train accuracy: 57.01541\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68203 | Train accuracy: 57.33526\n",
      "Epoch: 13\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68309 | Train accuracy: 56.87289\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68174 | Train accuracy: 57.41621\n",
      "Epoch: 14\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68274 | Train accuracy: 56.97978\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68181 | Train accuracy: 57.35734\n",
      "Epoch: 15\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68282 | Train accuracy: 56.92040\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68153 | Train accuracy: 57.43829\n",
      "Epoch: 16\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68294 | Train accuracy: 56.84913\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68159 | Train accuracy: 57.38678\n",
      "Epoch: 17\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68238 | Train accuracy: 57.03917\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68163 | Train accuracy: 57.34998\n",
      "Epoch: 18\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68250 | Train accuracy: 56.96791\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68153 | Train accuracy: 57.35734\n",
      "Epoch: 19\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68263 | Train accuracy: 56.89664\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68130 | Train accuracy: 57.42357\n",
      "Epoch: 20\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68225 | Train accuracy: 57.01541\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68132 | Train accuracy: 57.38678\n",
      "Epoch: 21\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68238 | Train accuracy: 56.94415\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68124 | Train accuracy: 57.39413\n",
      "Epoch: 22\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68244 | Train accuracy: 56.89664\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68132 | Train accuracy: 57.34262\n",
      "Epoch: 23\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68225 | Train accuracy: 56.94415\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68117 | Train accuracy: 57.37206\n",
      "Epoch: 24\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68212 | Train accuracy: 56.96791\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68109 | Train accuracy: 57.37942\n",
      "Epoch: 25\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68222 | Train accuracy: 56.90852\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68093 | Train accuracy: 57.41621\n",
      "Epoch: 26\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68232 | Train accuracy: 56.84913\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68095 | Train accuracy: 57.38678\n",
      "Epoch: 27\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68233 | Train accuracy: 56.82538\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68089 | Train accuracy: 57.38678\n",
      "Epoch: 28\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68199 | Train accuracy: 56.92040\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68062 | Train accuracy: 57.46037\n",
      "Epoch: 29\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68197 | Train accuracy: 56.90852\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68074 | Train accuracy: 57.39413\n",
      "Epoch: 30\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68186 | Train accuracy: 56.92040\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68067 | Train accuracy: 57.39413\n",
      "Epoch: 31\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68156 | Train accuracy: 57.00354\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68056 | Train accuracy: 57.40885\n",
      "Epoch: 32\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68149 | Train accuracy: 57.00354\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68053 | Train accuracy: 57.39413\n",
      "Epoch: 33\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68168 | Train accuracy: 56.90852\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68052 | Train accuracy: 57.37206\n",
      "Epoch: 34\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68154 | Train accuracy: 56.93227\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68029 | Train accuracy: 57.43093\n",
      "Epoch: 35\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68133 | Train accuracy: 56.97978\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68033 | Train accuracy: 57.38678\n",
      "Epoch: 36\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68142 | Train accuracy: 56.92040\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68030 | Train accuracy: 57.37206\n",
      "Epoch: 37\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68118 | Train accuracy: 56.97978\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68016 | Train accuracy: 57.39413\n",
      "Epoch: 38\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68113 | Train accuracy: 56.96791\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68018 | Train accuracy: 57.35734\n",
      "Epoch: 39\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68081 | Train accuracy: 57.05105\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68017 | Train accuracy: 57.33526\n",
      "Epoch: 40\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68087 | Train accuracy: 57.00354\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.68006 | Train accuracy: 57.34262\n",
      "Epoch: 41\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68111 | Train accuracy: 56.88477\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67963 | Train accuracy: 57.46773\n",
      "Epoch: 42\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68093 | Train accuracy: 56.92040\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67960 | Train accuracy: 57.44565\n",
      "Epoch: 43\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68066 | Train accuracy: 56.97978\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67962 | Train accuracy: 57.40885\n",
      "Epoch: 44\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68069 | Train accuracy: 56.93227\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67965 | Train accuracy: 57.36470\n",
      "Epoch: 45\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68054 | Train accuracy: 56.95603\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67938 | Train accuracy: 57.43093\n",
      "Epoch: 46\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68024 | Train accuracy: 57.02729\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67916 | Train accuracy: 57.47509\n",
      "Epoch: 47\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68005 | Train accuracy: 57.06292\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67933 | Train accuracy: 57.37942\n",
      "Epoch: 48\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68013 | Train accuracy: 56.99166\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67911 | Train accuracy: 57.42357\n",
      "Epoch: 49\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68020 | Train accuracy: 56.93227\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67878 | Train accuracy: 57.50453\n",
      "Epoch: 50\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67987 | Train accuracy: 57.01541\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67884 | Train accuracy: 57.44565\n",
      "Epoch: 51\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67987 | Train accuracy: 56.97978\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67892 | Train accuracy: 57.37942\n",
      "Epoch: 52\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.68006 | Train accuracy: 56.87289\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67876 | Train accuracy: 57.40149\n",
      "Epoch: 53\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67948 | Train accuracy: 57.03917\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67881 | Train accuracy: 57.34998\n",
      "Epoch: 54\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67936 | Train accuracy: 57.05105\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67851 | Train accuracy: 57.41621\n",
      "Epoch: 55\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67967 | Train accuracy: 56.89664\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67838 | Train accuracy: 57.42357\n",
      "Epoch: 56\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67949 | Train accuracy: 56.92040\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67840 | Train accuracy: 57.37942\n",
      "Epoch: 57\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67929 | Train accuracy: 56.94415\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67825 | Train accuracy: 57.38678\n",
      "Epoch: 58\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67911 | Train accuracy: 56.96791\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67803 | Train accuracy: 57.43093\n",
      "Epoch: 59\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67917 | Train accuracy: 56.89664\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67818 | Train accuracy: 57.33526\n",
      "Epoch: 60\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67876 | Train accuracy: 57.00354\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67796 | Train accuracy: 57.37206\n",
      "Epoch: 61\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67881 | Train accuracy: 56.94415\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67814 | Train accuracy: 57.26902\n",
      "Epoch: 62\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67870 | Train accuracy: 56.93227\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67759 | Train accuracy: 57.40885\n",
      "Epoch: 63\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67842 | Train accuracy: 56.99166\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67750 | Train accuracy: 57.40149\n",
      "Epoch: 64\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67861 | Train accuracy: 56.87289\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67739 | Train accuracy: 57.40149\n",
      "Epoch: 65\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67829 | Train accuracy: 56.93227\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67751 | Train accuracy: 57.31318\n",
      "Epoch: 66\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67792 | Train accuracy: 57.01541\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67741 | Train accuracy: 57.30582\n",
      "Epoch: 67\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67809 | Train accuracy: 56.90852\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67688 | Train accuracy: 57.44565\n",
      "Epoch: 68\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67797 | Train accuracy: 56.89664\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67695 | Train accuracy: 57.37206\n",
      "Epoch: 69\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67775 | Train accuracy: 56.92040\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67657 | Train accuracy: 57.45301\n",
      "Epoch: 70\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67756 | Train accuracy: 56.94415\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67654 | Train accuracy: 57.42357\n",
      "Epoch: 71\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67750 | Train accuracy: 56.90852\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67645 | Train accuracy: 57.40149\n",
      "Epoch: 72\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67719 | Train accuracy: 56.95603\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67635 | Train accuracy: 57.38678\n",
      "Epoch: 73\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67715 | Train accuracy: 56.92040\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67629 | Train accuracy: 57.34998\n",
      "Epoch: 74\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67686 | Train accuracy: 56.96791\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67595 | Train accuracy: 57.42357\n",
      "Epoch: 75\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67689 | Train accuracy: 56.89664\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67564 | Train accuracy: 57.47509\n",
      "Epoch: 76\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67693 | Train accuracy: 56.82538\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67576 | Train accuracy: 57.37942\n",
      "Epoch: 77\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67628 | Train accuracy: 56.99166\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67547 | Train accuracy: 57.43093\n",
      "Epoch: 78\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67606 | Train accuracy: 57.01541\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67543 | Train accuracy: 57.38678\n",
      "Epoch: 79\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67600 | Train accuracy: 56.96791\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67524 | Train accuracy: 57.39413\n",
      "Epoch: 80\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67590 | Train accuracy: 56.94415\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67510 | Train accuracy: 57.39413\n",
      "Epoch: 81\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67582 | Train accuracy: 56.89664\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67496 | Train accuracy: 57.37206\n",
      "Epoch: 82\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67558 | Train accuracy: 56.93227\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67467 | Train accuracy: 57.41621\n",
      "Epoch: 83\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67527 | Train accuracy: 56.96791\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67465 | Train accuracy: 57.35734\n",
      "Epoch: 84\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67521 | Train accuracy: 56.90852\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67426 | Train accuracy: 57.43093\n",
      "Epoch: 85\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67489 | Train accuracy: 56.95603\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67406 | Train accuracy: 57.43829\n",
      "Epoch: 86\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67457 | Train accuracy: 57.00354\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67409 | Train accuracy: 57.36470\n",
      "Epoch: 87\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67450 | Train accuracy: 56.95603\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67389 | Train accuracy: 57.36470\n",
      "Epoch: 88\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67412 | Train accuracy: 57.01541\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67365 | Train accuracy: 57.38678\n",
      "Epoch: 89\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67408 | Train accuracy: 56.95603\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67321 | Train accuracy: 57.46773\n",
      "Epoch: 90\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67392 | Train accuracy: 56.93227\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67304 | Train accuracy: 57.46037\n",
      "Epoch: 91\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67382 | Train accuracy: 56.88477\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67306 | Train accuracy: 57.38678\n",
      "Epoch: 92\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67334 | Train accuracy: 56.97978\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67286 | Train accuracy: 57.37206\n",
      "Epoch: 93\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67313 | Train accuracy: 56.96791\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67258 | Train accuracy: 57.39413\n",
      "Epoch: 94\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67304 | Train accuracy: 56.92040\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67240 | Train accuracy: 57.37942\n",
      "Epoch: 95\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67290 | Train accuracy: 56.88477\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67212 | Train accuracy: 57.40885\n",
      "Epoch: 96\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67270 | Train accuracy: 56.87289\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67191 | Train accuracy: 57.40149\n",
      "Epoch: 97\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67222 | Train accuracy: 56.95603\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67188 | Train accuracy: 57.33526\n",
      "Epoch: 98\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67188 | Train accuracy: 56.97978\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67155 | Train accuracy: 57.36470\n",
      "Epoch: 99\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "\n",
      "Train loss: 0.67168 | Train accuracy: 56.95603\n",
      "Looked at 0/1523 samples\n",
      "\n",
      "Train loss: 0.67117 | Train accuracy: 57.41621\n",
      "Train time on cpu: 10.726 seconds\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "epochs = 100\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_loss, train_accuracy = train_mode(model,train_dataloader,loss_fn,accuracy_fn,optimizer)\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Train accuracy: {train_accuracy:.5f}\")\n",
    "    test_loss, test_accuracy = test_mode(model,val_dataloader,loss_fn,accuracy_fn,optimizer)\n",
    "    print(f\"\\nTrain loss: {test_loss:.5f} | Train accuracy: {test_accuracy:.5f}\")\n",
    "\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9128c5b-8718-4774-b3a6-13862f7ce6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
