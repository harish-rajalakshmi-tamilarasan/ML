{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f51a534a-2a44-4954-961a-0c37db2a320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49de66fa-14e4-4bcc-847e-0d91a6088bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\harish-4072\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to the first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess_text_lemma(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8f4c59-fc50-47db-aa76-682a7109f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbc66f2-6378-429b-971a-41317e77d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\train.csv\")\n",
    "test_df = pd.read_csv(r\"D:\\Kaggle\\disaster tweets\\nlp-getting-started\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e28a093-b419-46eb-a352-960645ee1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}|(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},?\\s\\d{4})\\b'\n",
    "time_pattern = r'\\b((0?[1-9]|1[0-2]):[0-5]\\d\\s?(AM|PM)|([01]\\d|2[0-3]):[0-5]\\d(:[0-5]\\d)?)\\b'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'bin laden', 'Binladen', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", 'http', text, flags=re.MULTILINE)  \n",
    "    #text = re.sub(r'\\@\\w+|\\#','', text)  \n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b(?<!breaking)news\\b|\\b(?<!breaking)\\w*news\\w*\\b', 'news', text)\n",
    "    return text\n",
    "\n",
    "# train_df['text'] = train_df['location'].fillna('') + ' ' + train_df['text'].fillna('')\n",
    "# test_df['text'] = test_df['location'].fillna('') + ' ' + test_df['text'].fillna('')\n",
    "# train_df['text'] = train_df['keyword'].fillna('') + ' ' + train_df['text'].fillna('')\n",
    "# test_df['text'] = test_df['keyword'].fillna('') + ' ' + test_df['text'].fillna('')\n",
    "train_df['text'] = train_df['text'].apply(lambda x: re.sub(date_pattern, 'DATETIME', x))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: re.sub(time_pattern, 'DATETIME', x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: re.sub(date_pattern, 'DATETIME', x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: re.sub(time_pattern, 'DATETIME', x))\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text_lemma)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text_lemma)\n",
    "train_df['text'] = train_df['text'].apply(stemming)\n",
    "test_df['text'] = test_df['text'].apply(stemming)\n",
    "\n",
    "# train_df['url'] = train_df['text'].str.contains(r'http|https', regex=True)\n",
    "# test_df['url'] = test_df['text'].str.contains(r'http|https', regex=True)\n",
    "# train_df['contains_country'] = train_df['text'].str.contains(r'\\b(israel|afghan|iran|iraq|lebanon|yemen|palestine)\\b', regex=True, case=False)\n",
    "# test_df['contains_country'] = train_df['text'].str.contains(r'\\b(israel|afghan|iran|iraq|lebanon|yemen|palestine)\\b', regex=True, case=False)\n",
    "\n",
    "# train_df['isNews'] = train_df['text'].str.contains(r'news|News|Breakingnews|BreakingNews|breakingnews', regex=True)\n",
    "# test_df['isNews'] = test_df['text'].str.contains(r'news|News|Breakingnews|BreakingNews|breakingnews', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0501a7b1-6d02-448b-8de7-788a4935ec4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   id keyword location                                               text  \\\n",
       " 0   1     NaN      NaN  our deed be the reason of thi earthquak may al...   \n",
       " 1   4     NaN      NaN               forest fire near la rong sask canada   \n",
       " 2   5     NaN      NaN  all resid ask to shelter in place be be notifi...   \n",
       " 3   6     NaN      NaN     peopl receiv wildfir evacu order in california   \n",
       " 4   7     NaN      NaN  just get sent thi photo from rubi alaska a smo...   \n",
       " \n",
       "    target  \n",
       " 0       1  \n",
       " 1       1  \n",
       " 2       1  \n",
       " 3       1  \n",
       " 4       1  ,\n",
       " 38065,\n",
       " 13052,\n",
       " id             0\n",
       " keyword       61\n",
       " location    2533\n",
       " text           0\n",
       " target         0\n",
       " dtype: int64,\n",
       " id             0\n",
       " keyword       26\n",
       " location    1105\n",
       " text           0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(), train_df.size,test_df.size, train_df.isnull().sum(),test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb9bd0b-7f73-4775-8aa4-cf94a4b11baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_id = train_df['id']\n",
    "test_df_id = test_df['id']\n",
    "X = train_df['text']\n",
    "y = train_df['target']\n",
    "X_test = test_df['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c61f7a7-4e79-4962-a7cd-d4284813351f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         just happen a terribl car crash\n",
       "1       heard about earthquak be differ citi stay safe...\n",
       "2       there be a forest fire at spot pond gees be fl...\n",
       "3                          apocalyps light spokan wildfir\n",
       "4               typhoon soudelor kill in china and taiwan\n",
       "                              ...                        \n",
       "3258      earthquak safeti lo angel ûò safeti fasten xrwn\n",
       "3259    storm in ri bad than last hurrican my cityampo...\n",
       "3260                    green line derail in chicago http\n",
       "3261             meg issu hazard weather outlook hwo http\n",
       "3262    cityofcalgari have activ it municip emerg plan...\n",
       "Name: text, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b19f43-f813-4f25-acdf-450832f18caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "093bbfab-96a4-4ed9-a915-58f490d5b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X).toarray() \n",
    "y = y.values\n",
    "X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZES, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZES,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3acbfa75-e8ab-4692-aa85-f40f1957778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c27c9155-e573-45c1-aa83-a32290e4a580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bed6db1b-5b32-4c7d-ae8c-7ba68c07906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62d60434-4645-4b9c-bcb9-de23ac42b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDisasterModel(nn.Module):\n",
    "    def __init__(self,input_shape,hidden_units,out_shape):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape,out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units,out_features=out_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c292b425-92ab-4ce0-aeea-d5de5399b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_model = TweetDisasterModel(X_train.shape[1],8,1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(tweet_model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "541c8af4-7b1e-4782-aba0-cb28892b4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(all_labels, all_preds):\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy_score(all_labels, all_preds)),\n",
    "        'confusion_matrix': confusion_matrix(all_labels, all_preds),  # It's fine to leave the matrix as-is\n",
    "        'precision': float(precision_score(all_labels, all_preds)),\n",
    "        'recall': float(recall_score(all_labels, all_preds)),\n",
    "        'f1': float(f1_score(all_labels, all_preds)),\n",
    "        'macro_precision': float(precision_score(all_labels, all_preds, average='macro')),\n",
    "        'macro_recall': float(recall_score(all_labels, all_preds, average='macro')),\n",
    "        'macro_f1': float(f1_score(all_labels, all_preds, average='macro')),\n",
    "        'micro_precision': float(precision_score(all_labels, all_preds, average='micro')),\n",
    "        'micro_recall': float(recall_score(all_labels, all_preds, average='micro')),\n",
    "        'micro_f1': float(f1_score(all_labels, all_preds, average='micro'))\n",
    "    }\n",
    "    \n",
    "    return metrics, classification_report(all_labels, all_preds, target_names=['ham', 'spam'],digits = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efc8c6e0-9fe0-4537-8269-83714078e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mode(model: torch.nn.Module,data_loader: torch.utils.data.DataLoader, loss_fn:torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch, (X,y) in enumerate(data_loader):\n",
    "        y_preds = model(X)\n",
    "        loss = loss_fn(y_preds, y.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.sigmoid(y_preds).round()  # Apply sigmoid and threshold at 0.5\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        # running_accuracy +=\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(data_loader.dataset)} samples\")\n",
    "    train_loss = running_loss/len(data_loader)\n",
    "    \n",
    "    return train_loss, calculate_metrics(all_labels,all_preds)\n",
    "\n",
    "def test_mode(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch, (X,y) in enumerate(data_loader):\n",
    "        y_preds = model(X)\n",
    "        loss = loss_fn(y_preds, y.unsqueeze(1))\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.sigmoid(y_preds).round()  # Apply sigmoid and threshold at 0.5\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        if batch % 400 == 0:\n",
    "                print(f\"Looked at {batch * len(X)}/{len(data_loader.dataset)} samples\")\n",
    "    test_loss = running_loss/len(data_loader)\n",
    "    \n",
    "    return test_loss, calculate_metrics(all_labels,all_preds)\n",
    "\n",
    "def predict_on_test_set(model: torch.nn.Module, test_loader: torch.utils.data.DataLoader):\n",
    "    model.eval()  \n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients during inference\n",
    "        for batch, X in enumerate(test_loader):\n",
    "        \n",
    "            y_preds = model(X[0])\n",
    "            preds = torch.sigmoid(y_preds).round()  \n",
    "            all_preds.extend(preds.detach().cpu().numpy()) \n",
    "\n",
    "    return all_preds \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fd47258-414b-44af-85ac-c5405c58a385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8cf79fb349496eafb5cee076b1cc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.69264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.881306  0.085640  0.156110      3468\n",
      "        spam   0.448809  0.984744  0.616597      2622\n",
      "\n",
      "    accuracy                       0.472742      6090\n",
      "   macro avg   0.665057  0.535192  0.386354      6090\n",
      "weighted avg   0.695098  0.472742  0.354369      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.68174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.831008  0.613272  0.705727       874\n",
      "        spam   0.615034  0.832049  0.707269       649\n",
      "\n",
      "    accuracy                       0.706500      1523\n",
      "   macro avg   0.723021  0.722661  0.706498      1523\n",
      "weighted avg   0.738974  0.706500  0.706384      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 1\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.66227\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.808489  0.922722  0.861837      3468\n",
      "        spam   0.874296  0.710908  0.784182      2622\n",
      "\n",
      "    accuracy                       0.831527      6090\n",
      "   macro avg   0.841393  0.816815  0.823009      6090\n",
      "weighted avg   0.836822  0.831527  0.828403      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.65237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.735140  0.962243  0.833499       874\n",
      "        spam   0.912929  0.533128  0.673152       649\n",
      "\n",
      "    accuracy                       0.779383      1523\n",
      "   macro avg   0.824034  0.747685  0.753325      1523\n",
      "weighted avg   0.810902  0.779383  0.765170      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 2\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.62101\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.751923  0.986736  0.853473      3468\n",
      "        spam   0.970110  0.569413  0.717616      2622\n",
      "\n",
      "    accuracy                       0.807061      6090\n",
      "   macro avg   0.861017  0.778074  0.785544      6090\n",
      "weighted avg   0.845862  0.807061  0.794981      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.61817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.729239  0.964531  0.830542       874\n",
      "        spam   0.915531  0.517720  0.661417       649\n",
      "\n",
      "    accuracy                       0.774130      1523\n",
      "   macro avg   0.822385  0.741125  0.745980      1523\n",
      "weighted avg   0.808624  0.774130  0.758472      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 3\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.57350\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.769057  0.983276  0.863073      3468\n",
      "        spam   0.964976  0.609458  0.747078      2622\n",
      "\n",
      "    accuracy                       0.822332      6090\n",
      "   macro avg   0.867017  0.796367  0.805075      6090\n",
      "weighted avg   0.853408  0.822332  0.813132      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.58363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.750450  0.953089  0.839718       874\n",
      "        spam   0.900726  0.573190  0.700565       649\n",
      "\n",
      "    accuracy                       0.791202      1523\n",
      "   macro avg   0.825588  0.763139  0.770141      1523\n",
      "weighted avg   0.814488  0.791202  0.780420      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 4\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.52501\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.791482  0.980681  0.875982      3468\n",
      "        spam   0.962632  0.658276  0.781880      2622\n",
      "\n",
      "    accuracy                       0.841872      6090\n",
      "   macro avg   0.877057  0.819478  0.828931      6090\n",
      "weighted avg   0.865170  0.841872  0.835467      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.55124\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.762523  0.940503  0.842213       874\n",
      "        spam   0.883146  0.605547  0.718464       649\n",
      "\n",
      "    accuracy                       0.797768      1523\n",
      "   macro avg   0.822835  0.773025  0.780339      1523\n",
      "weighted avg   0.813925  0.797768  0.789480      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 5\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.47914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.816642  0.973472  0.888187      3468\n",
      "        spam   0.952965  0.710908  0.814329      2622\n",
      "\n",
      "    accuracy                       0.860427      6090\n",
      "   macro avg   0.884804  0.842190  0.851258      6090\n",
      "weighted avg   0.875335  0.860427  0.856388      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.52428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.770398  0.929062  0.842324       874\n",
      "        spam   0.867804  0.627119  0.728086       649\n",
      "\n",
      "    accuracy                       0.800394      1523\n",
      "   macro avg   0.819101  0.778090  0.785205      1523\n",
      "weighted avg   0.811906  0.800394  0.793643      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 6\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.43763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.835364  0.968570  0.897049      3468\n",
      "        spam   0.947318  0.747521  0.835643      2622\n",
      "\n",
      "    accuracy                       0.873399      6090\n",
      "   macro avg   0.891341  0.858045  0.866346      6090\n",
      "weighted avg   0.883565  0.873399  0.870611      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.50243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.781463  0.916476  0.843602       874\n",
      "        spam   0.853414  0.654854  0.741064       649\n",
      "\n",
      "    accuracy                       0.804990      1523\n",
      "   macro avg   0.817439  0.785665  0.792333      1523\n",
      "weighted avg   0.812124  0.804990  0.799907      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 7\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.40298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.852747  0.966840  0.906216      3468\n",
      "        spam   0.946710  0.779176  0.854812      2622\n",
      "\n",
      "    accuracy                       0.886043      6090\n",
      "   macro avg   0.899728  0.873008  0.880514      6090\n",
      "weighted avg   0.893202  0.886043  0.884084      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.48632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.790675  0.911899  0.846971       874\n",
      "        spam   0.850485  0.674884  0.752577       649\n",
      "\n",
      "    accuracy                       0.810900      1523\n",
      "   macro avg   0.820580  0.793392  0.799774      1523\n",
      "weighted avg   0.816162  0.810900  0.806747      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 8\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.37168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.866943  0.965686  0.913654      3468\n",
      "        spam   0.946565  0.803966  0.869458      2622\n",
      "\n",
      "    accuracy                       0.896059      6090\n",
      "   macro avg   0.906754  0.884826  0.891556      6090\n",
      "weighted avg   0.901223  0.896059  0.894626      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.47373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.795569  0.903890  0.846277       874\n",
      "        spam   0.841509  0.687211  0.756573       649\n",
      "\n",
      "    accuracy                       0.811556      1523\n",
      "   macro avg   0.818539  0.795551  0.801425      1523\n",
      "weighted avg   0.815146  0.811556  0.808052      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 9\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.34565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.872329  0.965398  0.916507      3468\n",
      "        spam   0.946714  0.813120  0.874846      2622\n",
      "\n",
      "    accuracy                       0.899836      6090\n",
      "   macro avg   0.909522  0.889259  0.895677      6090\n",
      "weighted avg   0.904355  0.899836  0.898570      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.46534\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.798146  0.886728  0.840108       874\n",
      "        spam   0.820652  0.697997  0.754371       649\n",
      "\n",
      "    accuracy                       0.806303      1523\n",
      "   macro avg   0.809399  0.792362  0.797240      1523\n",
      "weighted avg   0.807737  0.806303  0.803573      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 10\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.32331\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.884534  0.963091  0.922142      3468\n",
      "        spam   0.944685  0.833715  0.885737      2622\n",
      "\n",
      "    accuracy                       0.907389      6090\n",
      "   macro avg   0.914609  0.898403  0.903940      6090\n",
      "weighted avg   0.910431  0.907389  0.906469      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.45897\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.797131  0.890160  0.841081       874\n",
      "        spam   0.824497  0.694915  0.754181       649\n",
      "\n",
      "    accuracy                       0.806960      1523\n",
      "   macro avg   0.810814  0.792538  0.797631      1523\n",
      "weighted avg   0.808793  0.806960  0.804050      1523\n",
      "\n",
      "___________________________________\n",
      "Epoch: 11\n",
      "---------\n",
      "Looked at 0/6090 samples\n",
      "Train loss: 0.30412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.889391  0.964533  0.925439      3468\n",
      "        spam   0.947188  0.841342  0.891133      2622\n",
      "\n",
      "    accuracy                       0.911494      6090\n",
      "   macro avg   0.918289  0.902938  0.908286      6090\n",
      "weighted avg   0.914275  0.911494  0.910669      6090\n",
      "\n",
      "Looked at 0/1523 samples\n",
      "Test loss: 0.45363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham   0.797116  0.885584  0.839024       874\n",
      "        spam   0.818841  0.696456  0.752706       649\n",
      "\n",
      "    accuracy                       0.804990      1523\n",
      "   macro avg   0.807978  0.791020  0.795865      1523\n",
      "weighted avg   0.806374  0.804990  0.802241      1523\n",
      "\n",
      "___________________________________\n",
      "Train time on cpu: 8.058 seconds\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Start timer\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 12\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_loss, (train_metrics, train_classification_report)= train_mode(tweet_model, train_loader, criterion, optimizer)\n",
    "    print(f\"Train loss: {train_loss:.5f}\")\n",
    "    # print(f\"Train metrics: {train_metrics}\")\n",
    "    print(train_classification_report)\n",
    "    \n",
    "    # Test/Validate the model\n",
    "    test_loss, (test_metrics,test_classification_report) = test_mode(tweet_model, val_loader, criterion, optimizer)\n",
    "    print(f\"Test loss: {test_loss:.5f}\")\n",
    "    # print(f\"Test metrics: {test_metrics}\")\n",
    "    print(test_classification_report)\n",
    "\n",
    "    print(\"___________________________________\")\n",
    "    \n",
    "# End timer\n",
    "train_time_end_on_cpu = timer()\n",
    "\n",
    "# Calculate total training time\n",
    "total_train_time_model = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(tweet_model.parameters()).device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36dcb485-a503-479a-a390-0da859431eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_on_test_set(tweet_model, test_loader)\n",
    "y_pred = [int(pred[0]) for pred in y_pred]\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_df_id,\n",
    "    'target': y_pred\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv(r'D:\\Kaggle\\disaster tweets\\simple_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506e038-44db-4a70-8e3a-bcd82cb3014b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
